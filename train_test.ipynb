{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 20:52:19.555505: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 20:52:20.097977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-16 20:52:20.098017: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-16 20:52:20.098022: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/jun/miniconda3/envs/tf2/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset imdb (/home/jun/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n",
      "100%|██████████| 3/3 [00:00<00:00, 675.70it/s]\n",
      "Loading cached processed dataset at /home/jun/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-cf30a45638af7fe4.arrow\n",
      "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "2023-10-16 20:52:29.131943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.132346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.137732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.138082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.138413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.138739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.139337: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 20:52:29.355062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.355378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.355642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.355892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.356142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.356390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.930306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.930636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.930905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.931158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.931407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.931643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-16 20:52:29.931927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 20:52:29.932156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "Loading cached processed dataset at /home/jun/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0/cache-9de985f101612c32.arrow\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import create_optimizer,TransfoXLTokenizer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from transformers.file_utils import (\n",
    "    ModelOutput,\n",
    "    add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    ")\n",
    "from transformers.modeling_tf_utils import (\n",
    "    TFPreTrainedModel,\n",
    "    TFSequenceClassificationLoss,\n",
    "    get_initializer,\n",
    "    input_processing,\n",
    "    keras_serializable,\n",
    "    shape_list,\n",
    ")\n",
    "from transformers import logging, AutoTokenizer\n",
    "from transformers.models.transfo_xl.configuration_transfo_xl import TransfoXLConfig\n",
    "from transformers.models.transfo_xl.modeling_tf_transfo_xl_utilities import TFAdaptiveSoftmaxMask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from datasets import load_dataset\n",
    "# from transformers import DataCollatorWithPadding\n",
    "\n",
    "# # gpus = tf.config.list_logical_devices('GPU')\n",
    "# # strategy = tf.distribute.MirroredStrategy(gpus)\n",
    "\n",
    "# imdb = load_dataset(\"imdb\")\n",
    "\n",
    "# tokenizer = TransfoXLTokenizer.from_pretrained(\"transfo-xl-wt103\")\n",
    "# tokenizer.bos_token=\"[CLS]\"\n",
    "# tokenizer.eos_token=\"[SEP]\"\n",
    "# tokenizer.sep_token=\"[SEP]\"\n",
    "# tokenizer.cls_token=\"[CLS]\"\n",
    "# tokenizer.unk_token=\"[UNK]\"\n",
    "# tokenizer.pad_token=\"[PAD]\"      \n",
    "        \n",
    "# tokenizer._bos_token=\"[CLS]\"\n",
    "# tokenizer._eos_token=\"[SEP]\"\n",
    "# tokenizer._sep_token=\"[SEP]\"\n",
    "# tokenizer._cls_token=\"[CLS]\"\n",
    "# tokenizer._unk_token=\"[UNK]\"\n",
    "# tokenizer._pad_token=\"[PAD]\"\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     return tokenizer(examples[\"text\"],padding=True,truncation=True,return_tensors='tf')\n",
    "\n",
    "# tokenized_imdb = imdb.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_imdb.save_to_disk('/home/jun/workspace/transfo_xl/data')\n",
    "\n",
    "tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "tokenized_imdb = load_from_disk(\"/home/jun/workspace/transfo_xl/data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label', 'input_ids'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens =tokenizer.encode_plus('hello world',\n",
    "                              truncation=True,\n",
    "                              add_special_tokens=True, return_token_type_ids=False,\n",
    "                              return_attention_mask=True,return_tensors='tf') #truncation 길이를 초과할 경우 자르는지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=TransfoXLTokenizer(name_or_path='transfo-xl-wt103', vocab_size=267735, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<eos>', 'unk_token': '<unk>', 'additional_special_tokens': ['<formula>']}, clean_up_tokenization_spaces=True), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='tf')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 16:54:38.899767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.901278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.915577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.916305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.916910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.917526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:38.919467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 16:54:39.109511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.109825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.110089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.110342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.110594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.110845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.681337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.681677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.681946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.682200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.682450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.682687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-16 16:54:39.683007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-16 16:54:39.683244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "_CHECKPOINT_FOR_DOC = \"transfo-xl-wt103\"\n",
    "_CONFIG_FOR_DOC = \"TransfoXLConfig\"\n",
    "_TOKENIZER_FOR_DOC = \"TransfoXLTokenizer\"\n",
    "\n",
    "TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
    "    \"transfo-xl-wt103\",\n",
    "    # See all Transformer XL models at https://huggingface.co/models?filter=transfo-xl\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "class TFPositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, demb, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))\n",
    "\n",
    "    def call(self, pos_seq, bsz=None):\n",
    "        self.inv_freq = tf.cast(self.inv_freq, dtype=pos_seq.dtype)\n",
    "        sinusoid_inp = tf.einsum(\"i,j->ij\", pos_seq, self.inv_freq)\n",
    "        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
    "\n",
    "        if bsz is not None:\n",
    "            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
    "        else:\n",
    "            return pos_emb[:, None, :]\n",
    "        \n",
    "\n",
    "class TFPositionwiseFF(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5, init_std=0.02, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layer_1 = tf.keras.layers.Dense(\n",
    "            d_inner, kernel_initializer=get_initializer(init_std), activation=tf.nn.relu, name=\"CoreNet_._0\"\n",
    "        )\n",
    "        self.drop_1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=get_initializer(init_std), name=\"CoreNet_._3\")\n",
    "        self.drop_2 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "    def call(self, inp, training=False):\n",
    "        if self.pre_lnorm:\n",
    "            # layer normalization + positionwise feed-forward\n",
    "            core_out = self.layer_norm(inp)\n",
    "            core_out = self.layer_1(core_out)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection\n",
    "            output = core_out + inp\n",
    "        else:\n",
    "            # positionwise feed-forward\n",
    "            core_out = self.layer_1(inp)\n",
    "            core_out = self.drop_1(core_out, training=training)\n",
    "            core_out = self.layer_2(core_out)\n",
    "            core_out = self.drop_2(core_out, training=training)\n",
    "\n",
    "            # residual connection + layer normalization\n",
    "            output = self.layer_norm(inp + core_out)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class TFRelPartialLearnableMultiHeadAttn(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_r_bias=None,\n",
    "        r_w_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_head = d_head\n",
    "        self.dropout = dropout\n",
    "        self.output_attentions = output_attentions\n",
    "\n",
    "        self.qkv_net = tf.keras.layers.Dense(\n",
    "            3 * n_head * d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"qkv_net\"\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(dropout)\n",
    "        self.dropatt = tf.keras.layers.Dropout(dropatt)\n",
    "        self.o_net = tf.keras.layers.Dense(\n",
    "            d_model, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"o_net\"\n",
    "        )\n",
    "\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=layer_norm_epsilon, name=\"layer_norm\")\n",
    "\n",
    "        self.scale = 1 / (d_head ** 0.5)\n",
    "\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "\n",
    "        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n",
    "            self.r_r_bias = r_r_bias\n",
    "            self.r_w_bias = r_w_bias\n",
    "        else:\n",
    "            self.r_r_bias = None\n",
    "            self.r_w_bias = None\n",
    "\n",
    "        self.r_net = tf.keras.layers.Dense(\n",
    "            self.n_head * self.d_head, kernel_initializer=get_initializer(init_std), use_bias=False, name=\"r_net\"\n",
    "        )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.r_r_bias is None or self.r_w_bias is None:  # Biases are not shared\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _rel_shift(self, x):\n",
    "        x_size = shape_list(x)\n",
    "\n",
    "        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
    "        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
    "        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
    "        x = tf.reshape(x, x_size)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n",
    "\n",
    "        if mems is not None:\n",
    "            mems = tf.cast(mems, dtype=w.dtype)\n",
    "            cat = tf.concat([mems, w], 0)\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(cat))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(cat)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "            w_head_q = w_head_q[-qlen:]\n",
    "        else:\n",
    "            if self.pre_lnorm:\n",
    "                w_heads = self.qkv_net(self.layer_norm(w))\n",
    "            else:\n",
    "                w_heads = self.qkv_net(w)\n",
    "            r_head_k = self.r_net(r)\n",
    "\n",
    "            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
    "\n",
    "        klen = shape_list(w_head_k)[0]\n",
    "\n",
    "        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))  # qlen x bsz x n_head x d_head\n",
    "\n",
    "        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))  # qlen x n_head x d_head\n",
    "\n",
    "        # compute attention score\n",
    "        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head\n",
    "        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)  # qlen x klen x bsz x n_head\n",
    "\n",
    "        rr_head_q = w_head_q + self.r_r_bias\n",
    "        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)  # qlen x klen x bsz x n_head\n",
    "        BD = self._rel_shift(BD)\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_score = AC + BD\n",
    "        attn_score = attn_score * self.scale\n",
    "\n",
    "        # compute attention probability\n",
    "        if attn_mask is not None:\n",
    "            attn_mask_t = attn_mask[:, :, None, None]\n",
    "            attn_mask_t = tf.cast(attn_mask_t, dtype=attn_score.dtype)\n",
    "            attn_score = attn_score * (1.0 - attn_mask_t) - 1e30 * attn_mask_t\n",
    "\n",
    "        # [qlen x klen x bsz x n_head]\n",
    "        attn_prob = tf.nn.softmax(attn_score, axis=1)\n",
    "        attn_prob = self.dropatt(attn_prob, training=training)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attn_prob = attn_prob * head_mask\n",
    "\n",
    "        # compute attention vector\n",
    "        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)\n",
    "\n",
    "        # [qlen x bsz x n_head x d_head]\n",
    "        attn_vec_sizes = shape_list(attn_vec)\n",
    "        attn_vec = tf.reshape(attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head))\n",
    "\n",
    "        # linear projection\n",
    "        attn_out = self.o_net(attn_vec)\n",
    "        attn_out = self.drop(attn_out, training=training)\n",
    "\n",
    "        if self.pre_lnorm:\n",
    "            # residual connection\n",
    "            outputs = [w + attn_out]\n",
    "        else:\n",
    "            # residual connection + layer normalization\n",
    "            outputs = [self.layer_norm(w + attn_out)]\n",
    "\n",
    "        if output_attentions:\n",
    "            outputs.append(attn_prob)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "\n",
    "class TFRelPartialLearnableDecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        d_model,\n",
    "        d_head,\n",
    "        d_inner,\n",
    "        dropout,\n",
    "        dropatt=0.0,\n",
    "        pre_lnorm=False,\n",
    "        r_w_bias=None,\n",
    "        r_r_bias=None,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        init_std=0.02,\n",
    "        output_attentions=False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dec_attn = TFRelPartialLearnableMultiHeadAttn(\n",
    "            n_head,\n",
    "            d_model,\n",
    "            d_head,\n",
    "            dropout,\n",
    "            dropatt=dropatt,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            r_w_bias=r_w_bias,\n",
    "            r_r_bias=r_r_bias,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            output_attentions=output_attentions,\n",
    "            name=\"dec_attn\",\n",
    "        )\n",
    "        self.pos_ff = TFPositionwiseFF(\n",
    "            d_model,\n",
    "            d_inner,\n",
    "            dropout,\n",
    "            pre_lnorm=pre_lnorm,\n",
    "            init_std=init_std,\n",
    "            layer_norm_epsilon=layer_norm_epsilon,\n",
    "            name=\"pos_ff\",\n",
    "        )\n",
    "\n",
    "    def call(self, dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=False):\n",
    "        attn_outputs = self.dec_attn(dec_inp, r, dec_attn_mask, mems, head_mask, output_attentions, training=training)\n",
    "        ff_output = self.pos_ff(attn_outputs[0], training=training)\n",
    "\n",
    "        outputs = [ff_output] + attn_outputs[1:]\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "class TFTransfoEmbeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, emb_size, init_std, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_size = emb_size\n",
    "        self.init_std = init_std\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weight = self.add_weight(\n",
    "            shape=(self.vocab_size, self.emb_size),\n",
    "            initializer=get_initializer(self.init_std),\n",
    "            name=\"embeddings\",\n",
    "        )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.gather(self.weight, inputs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TFAdaptiveEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1, init_std=0.02, sample_softmax=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_token = n_token\n",
    "        self.d_embed = d_embed\n",
    "        self.init_std = init_std\n",
    "\n",
    "        self.cutoffs = cutoffs + [n_token]\n",
    "        self.div_val = div_val\n",
    "        self.d_proj = d_proj\n",
    "\n",
    "        self.emb_scale = d_proj ** 0.5\n",
    "\n",
    "        self.cutoff_ends = [0] + self.cutoffs\n",
    "\n",
    "        self.emb_layers = []\n",
    "        self.emb_projs = []\n",
    "\n",
    "        if div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "                d_emb_i = d_embed // (div_val ** i)\n",
    "                self.emb_layers.append(\n",
    "                    TFTransfoEmbeddings(\n",
    "                        r_idx - l_idx,\n",
    "                        d_emb_i,\n",
    "                        init_std,\n",
    "                        name=f\"emb_layers_._{i}\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for i in range(len(self.cutoffs)):\n",
    "            d_emb_i = self.d_embed // (self.div_val ** i)\n",
    "            self.emb_projs.append(\n",
    "                self.add_weight(\n",
    "                    shape=(d_emb_i, self.d_proj),\n",
    "                    initializer=get_initializer(self.init_std),\n",
    "                    trainable=True,\n",
    "                    name=f\"emb_projs_._{i}\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inp):\n",
    "        if self.div_val == 1:\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "        else:\n",
    "            inp_flat = tf.reshape(inp, (-1,))\n",
    "            emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])\n",
    "            for i in range(len(self.cutoffs)):\n",
    "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
    "\n",
    "                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n",
    "\n",
    "                inp_i = tf.boolean_mask(inp_flat, mask_i) - l_idx\n",
    "                emb_i = self.emb_layers[i](inp_i)\n",
    "                emb_i = tf.einsum(\"id,de->ie\", emb_i, self.emb_projs[i])\n",
    "\n",
    "                mask_idx = tf.where(mask_i)\n",
    "                scatter = tf.scatter_nd(mask_idx, emb_i, shape_list(emb_flat))\n",
    "                emb_flat = tf.cast(emb_flat, dtype=scatter.dtype)\n",
    "                emb_flat += scatter\n",
    "\n",
    "            embed_shape = shape_list(inp) + [self.d_proj]\n",
    "            embed = tf.reshape(emb_flat, embed_shape)\n",
    "\n",
    "        embed *= self.emb_scale\n",
    "\n",
    "        return embed\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "@keras_serializable\n",
    "class TFTransfoXLMainLayer(tf.keras.layers.Layer):\n",
    "    config_class = TransfoXLConfig\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.config = config\n",
    "        self.output_hidden_states = config.output_hidden_states\n",
    "        self.output_attentions = config.output_attentions\n",
    "        self.return_dict = config.use_return_dict\n",
    "\n",
    "        self.n_token = config.vocab_size\n",
    "\n",
    "        self.d_embed = config.d_embed\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "        self.untie_r = config.untie_r\n",
    "\n",
    "        self.word_emb = TFAdaptiveEmbedding(\n",
    "            config.vocab_size,\n",
    "            config.d_embed,\n",
    "            config.d_model,\n",
    "            config.cutoffs,\n",
    "            div_val=config.div_val,\n",
    "            init_std=config.init_std,\n",
    "            name=\"word_emb\",\n",
    "        )\n",
    "\n",
    "        self.drop = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "        self.n_layer = config.n_layer\n",
    "        self.mem_len = config.mem_len\n",
    "        self.attn_type = config.attn_type\n",
    "\n",
    "        self.layers = []\n",
    "        if config.attn_type == 0:  # the default attention\n",
    "            for i in range(config.n_layer):\n",
    "                self.layers.append(\n",
    "                    TFRelPartialLearnableDecoderLayer(\n",
    "                        config.n_head,\n",
    "                        config.d_model,\n",
    "                        config.d_head,\n",
    "                        config.d_inner,\n",
    "                        config.dropout,\n",
    "                        dropatt=config.dropatt,\n",
    "                        pre_lnorm=config.pre_lnorm,\n",
    "                        r_w_bias=None if self.untie_r else self.r_w_bias,\n",
    "                        r_r_bias=None if self.untie_r else self.r_r_bias,\n",
    "                        layer_norm_epsilon=config.layer_norm_epsilon,\n",
    "                        init_std=config.init_std,\n",
    "                        output_attentions=self.output_attentions,\n",
    "                        name=f\"layers_._{i}\",\n",
    "                    )\n",
    "                )\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "        self.same_length = config.same_length\n",
    "        self.clamp_len = config.clamp_len\n",
    "\n",
    "        if self.attn_type == 0:  # default attention\n",
    "            self.pos_emb = TFPositionalEmbedding(self.d_model, name=\"pos_emb\")\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not self.untie_r:\n",
    "            self.r_w_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
    "            )\n",
    "            self.r_r_bias = self.add_weight(\n",
    "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.word_emb\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward_compatible(self):\n",
    "        self.sample_softmax = -1\n",
    "\n",
    "    def reset_memory_length(self, mem_len):\n",
    "        self.mem_len = mem_len\n",
    "\n",
    "    def _prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def init_mems(self, bsz):\n",
    "        if self.mem_len > 0:\n",
    "            mems = []\n",
    "            for i in range(self.n_layer):\n",
    "                empty = tf.zeros([self.mem_len, bsz, self.d_model])\n",
    "                mems.append(empty)\n",
    "\n",
    "            return mems\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _update_mems(self, hids, mems, mlen, qlen):\n",
    "        # does not deal with None\n",
    "        if mems is None:\n",
    "            return None\n",
    "\n",
    "        # mems is not None\n",
    "        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n",
    "\n",
    "        # There are `mlen + qlen` steps that can be cached into mems\n",
    "        new_mems = []\n",
    "        end_idx = mlen + tf.math.maximum(0, qlen)\n",
    "        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))\n",
    "        for i in range(len(hids)):\n",
    "            mems[i] = tf.cast(mems[i], dtype=hids[i].dtype)\n",
    "            cat = tf.concat([mems[i], hids[i]], axis=0)\n",
    "            tf.stop_gradient(cat)\n",
    "            new_mems.append(cat[beg_idx:end_idx])\n",
    "\n",
    "        return new_mems\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            mems=mems,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n",
    "        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n",
    "        if inputs[\"input_ids\"] is not None and inputs[\"inputs_embeds\"] is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif inputs[\"input_ids\"] is not None:\n",
    "            inputs[\"input_ids\"] = tf.transpose(inputs[\"input_ids\"], perm=(1, 0))\n",
    "            qlen, bsz = shape_list(inputs[\"input_ids\"])\n",
    "        elif inputs[\"inputs_embeds\"] is not None:\n",
    "            inputs[\"inputs_embeds\"] = tf.transpose(inputs[\"inputs_embeds\"], perm=(1, 0, 2))\n",
    "            qlen, bsz = shape_list(inputs[\"inputs_embeds\"])[:2]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs[\"mems\"] is None:\n",
    "            inputs[\"mems\"] = self.init_mems(bsz)\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n",
    "        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n",
    "        if inputs[\"head_mask\"] is not None:\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            inputs[\"head_mask\"] = [None] * self.n_layer\n",
    "\n",
    "        if inputs[\"inputs_embeds\"] is not None:\n",
    "            word_emb = inputs[\"inputs_embeds\"]\n",
    "        else:\n",
    "            word_emb = self.word_emb(inputs[\"input_ids\"])\n",
    "\n",
    "        mlen = shape_list(inputs[\"mems\"][0])[0] if inputs[\"mems\"] is not None else 0\n",
    "        klen = mlen + qlen\n",
    "\n",
    "        attn_mask = tf.ones([qlen, qlen])\n",
    "        mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
    "        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
    "        attn_mask_pad = tf.zeros([qlen, mlen])\n",
    "        dec_attn_mask = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
    "        if self.same_length:\n",
    "            mask_l = tf.linalg.band_part(attn_mask, -1, 0)\n",
    "            dec_attn_mask = tf.concat([dec_attn_mask[:, :qlen] + mask_l - mask_dia, dec_attn_mask[:, qlen:]], 1)\n",
    "        # ::: PyTorch masking code for reference :::\n",
    "        # if self.same_length:\n",
    "        #     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)\n",
    "        #     mask_len = klen - self.mem_len\n",
    "        #     if mask_len > 0:\n",
    "        #         mask_shift_len = qlen - mask_len\n",
    "        #     else:\n",
    "        #         mask_shift_len = qlen\n",
    "        #     dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n",
    "        #             + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1\n",
    "        # else:\n",
    "        #     dec_attn_mask = torch.triu(\n",
    "        #         word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]\n",
    "\n",
    "        hids = []\n",
    "        attentions = [] if inputs[\"output_attentions\"] else None\n",
    "        if self.attn_type == 0:  # default\n",
    "            pos_seq = tf.range(klen - 1, -1, -1.0)\n",
    "            if self.clamp_len > 0:\n",
    "                pos_seq = tf.minimum(pos_seq, self.clamp_len)\n",
    "            pos_emb = self.pos_emb(pos_seq)\n",
    "\n",
    "            core_out = self.drop(word_emb, training=inputs[\"training\"])\n",
    "            pos_emb = self.drop(pos_emb, training=inputs[\"training\"])\n",
    "\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                hids.append(core_out)\n",
    "                mems_i = None if inputs[\"mems\"] is None else inputs[\"mems\"][i]\n",
    "                layer_outputs = layer(\n",
    "                    core_out,\n",
    "                    pos_emb,\n",
    "                    dec_attn_mask,\n",
    "                    mems_i,\n",
    "                    inputs[\"head_mask\"][i],\n",
    "                    inputs[\"output_attentions\"],\n",
    "                    training=inputs[\"training\"],\n",
    "                )\n",
    "                core_out = layer_outputs[0]\n",
    "                if inputs[\"output_attentions\"]:\n",
    "                    attentions.append(layer_outputs[1])\n",
    "        else:  # learnable embeddings and absolute embeddings\n",
    "            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n",
    "\n",
    "        core_out = self.drop(core_out, training=inputs[\"training\"])\n",
    "\n",
    "        new_mems = self._update_mems(hids, inputs[\"mems\"], mlen, qlen)\n",
    "\n",
    "        # We transpose back here to shape [bsz, len, hidden_dim]\n",
    "        core_out = tf.transpose(core_out, perm=(1, 0, 2))\n",
    "\n",
    "        if inputs[\"output_hidden_states\"]:\n",
    "            # Transpose to library standard shape [bsz, len, hidden_dim] and add last layer\n",
    "            hids = tuple(tf.transpose(t, perm=(1, 0, 2)) for t in hids)\n",
    "            hids = hids + (core_out,)\n",
    "        else:\n",
    "            hids = None\n",
    "        if inputs[\"output_attentions\"]:\n",
    "            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]\n",
    "            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return tuple(v for v in [core_out, new_mems, hids, attentions] if v is not None)\n",
    "\n",
    "        return TFTransfoXLModelOutput(\n",
    "            last_hidden_state=core_out,\n",
    "            mems=new_mems,\n",
    "            hidden_states=hids,\n",
    "            attentions=attentions,\n",
    "        )\n",
    "        \n",
    "\n",
    "class TFTransfoXLPreTrainedModel(TFPreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = TransfoXLConfig\n",
    "    base_model_prefix = \"transformer\"\n",
    "\n",
    "    @tf.function(\n",
    "        input_signature=[\n",
    "            {\n",
    "                \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    def serving(self, inputs):\n",
    "        output = self.call(inputs)\n",
    "\n",
    "        return self.serving_output(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "@dataclass\n",
    "class TFTransfoXLModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n",
    "            Sequence of hidden-states at the output of the last layer of the model.\n",
    "        mems (`List[tf.Tensor]` of length `config.n_layers`):\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`\n",
    "            input) to speed up sequential decoding. The token ids which have their past given to this model should not\n",
    "            be passed as input ids as they have already been computed.\n",
    "        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    last_hidden_state: tf.Tensor = None\n",
    "    mems: List[tf.Tensor] = None\n",
    "    hidden_states: Optional[Tuple[tf.Tensor]] = None\n",
    "    attentions: Optional[Tuple[tf.Tensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFTransfoXLLMHeadModelOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n",
    "\n",
    "    Args:\n",
    "        losses (`tf.Tensor` of shape *(batch_size, sequence_length-1)*, *optional*, returned when `labels` is provided)\n",
    "            Language modeling losses (not reduced).\n",
    "        prediction_scores (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n",
    "            Prediction scores of the language modeling head (scores for each vocabulary token after SoftMax).\n",
    "        mems (`List[tf.Tensor]` of length `config.n_layers`):\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`\n",
    "            input) to speed up sequential decoding. The token ids which have their past given to this model should not\n",
    "            be passed as input ids as they have already been computed.\n",
    "        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    prediction_scores: tf.Tensor = None\n",
    "    mems: List[tf.Tensor] = None\n",
    "    hidden_states: Optional[Tuple[tf.Tensor]] = None\n",
    "    attentions: Optional[Tuple[tf.Tensor]] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TFTransfoXLSequenceClassifierOutputWithPast(ModelOutput):\n",
    "    \"\"\"\n",
    "    Base class for outputs of sentence classification models.\n",
    "\n",
    "    Args:\n",
    "        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        mems (`List[tf.Tensor]` of length `config.n_layers`):\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`\n",
    "            input) to speed up sequential decoding. The token ids which have their past given to this model should not\n",
    "            be passed as input ids as they have already been computed.\n",
    "        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
    "            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of\n",
    "            shape `(batch_size, sequence_length, hidden_size)`.\n",
    "\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n",
    "            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.\n",
    "\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n",
    "            heads.\n",
    "    \"\"\"\n",
    "\n",
    "    loss: Optional[tf.Tensor] = None\n",
    "    logits: tf.Tensor = None\n",
    "    mems: List[tf.Tensor] = None\n",
    "    hidden_states: Optional[Tuple[tf.Tensor]] = None\n",
    "    attentions: Optional[Tuple[tf.Tensor]] = None\n",
    "    \n",
    "    \n",
    "\n",
    "TRANSFO_XL_START_DOCSTRING = r\"\"\"\n",
    "\n",
    "    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the\n",
    "    generic methods the library implements for all its model (such as downloading or saving, resizing the input\n",
    "    embeddings, pruning heads etc.)\n",
    "\n",
    "    This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use\n",
    "    it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage\n",
    "    and behavior.\n",
    "\n",
    "    <Tip>\n",
    "\n",
    "    TF 2.0 models accepts two formats as inputs:\n",
    "\n",
    "    - having all inputs as keyword arguments (like PyTorch models), or\n",
    "    - having all inputs as a list, tuple or dict in the first positional arguments.\n",
    "\n",
    "    This second option is useful when using [`tf.keras.Model.fit`] method which currently requires having all\n",
    "    the tensors in the first argument of the model call function: `model(inputs)`.\n",
    "\n",
    "    If you choose this second option, there are three possibilities you can use to gather all the input Tensors in\n",
    "    the first positional argument :\n",
    "\n",
    "    - a single Tensor with `input_ids` only and nothing else: `model(inputs_ids)`\n",
    "    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:\n",
    "    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`\n",
    "    - a dictionary with one or several input Tensors associated to the input names given in the docstring:\n",
    "    `model({\"input_ids\": input_ids, \"token_type_ids\": token_type_ids})`\n",
    "\n",
    "    </Tip>\n",
    "\n",
    "    Parameters:\n",
    "        config ([`TransfoXLConfig`]): Model configuration class with all the parameters of the model.\n",
    "            Initializing with a config file does not load the weights associated with the model, only the\n",
    "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model\n",
    "            weights.\n",
    "\"\"\"\n",
    "\n",
    "TRANSFO_XL_INPUTS_DOCSTRING = r\"\"\"\n",
    "    Args:\n",
    "        input_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`):\n",
    "            Indices of input sequence tokens in the vocabulary.\n",
    "\n",
    "            Indices can be obtained using [`BertTokenizer`]. See\n",
    "            [`PreTrainedTokenizer.__call__`] and [`PreTrainedTokenizer.encode`] for\n",
    "            details.\n",
    "\n",
    "            [What are input IDs?](../glossary#input-ids)\n",
    "        mems (`List[tf.Tensor]` of length `config.n_layers`):\n",
    "            Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (see\n",
    "            `mems` output below). Can be used to speed up sequential decoding. The token ids which have their mems\n",
    "            given to this model should not be passed as `input_ids` as they have already been computed.\n",
    "        head_mask (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n",
    "            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n",
    "\n",
    "            - 1 indicates the head is **not masked**,\n",
    "            - 0 indicates the head is **masked**.\n",
    "        inputs_embeds (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "            This is useful if you want more control over how to convert `input_ids` indices into associated\n",
    "            vectors than the model's internal embedding lookup matrix.\n",
    "        output_attentions (`bool`, *optional*):\n",
    "            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
    "            tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the\n",
    "            config will be used instead.\n",
    "        output_hidden_states (`bool`, *optional*):\n",
    "            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
    "            more detail. This argument can be used only in eager mode, in graph mode the value in the config will be\n",
    "            used instead.\n",
    "        return_dict (`bool`, *optional*):\n",
    "            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple. This\n",
    "            argument can be used in eager mode, in graph mode the value will always be set to True.\n",
    "        training (`bool`, *optional*, defaults to `False`):\n",
    "            Whether or not to use the model in training mode (some modules like dropout modules have different\n",
    "            behaviors between training and evaluation).\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n",
    "    TRANSFO_XL_START_DOCSTRING,\n",
    ")\n",
    "class TFTransfoXLModel(TFTransfoXLPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs): # *inputs 란 config 다음 자리에 여러개의 인수가 올 수 있음을 의미 \n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.transformer = TFTransfoXLMainLayer(config, name=\"transformer\")\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TFTransfoXLModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            mems=mems,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "        outputs = self.transformer(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            mems=inputs[\"mems\"],\n",
    "            head_mask=inputs[\"head_mask\"],\n",
    "            inputs_embeds=inputs[\"inputs_embeds\"],\n",
    "            output_attentions=inputs[\"output_attentions\"],\n",
    "            output_hidden_states=inputs[\"output_hidden_states\"],\n",
    "            return_dict=inputs[\"return_dict\"],\n",
    "            training=inputs[\"training\"],\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def serving_output(self, output):\n",
    "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\n",
    "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\n",
    "\n",
    "        return TFTransfoXLModelOutput(\n",
    "            last_hidden_state=output.last_hidden_state,\n",
    "            mems=tf.convert_to_tensor(output.mems),\n",
    "            hidden_states=hs,\n",
    "            attentions=attns,\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    The Transformer-XL Model with a language modeling head on top (adaptive softmax with weights tied to the adaptive\n",
    "    input embeddings)\n",
    "    \"\"\",\n",
    "    TRANSFO_XL_START_DOCSTRING,\n",
    ")\n",
    "class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = TFTransfoXLMainLayer(config, name=\"transformer\")\n",
    "        self.sample_softmax = config.sample_softmax\n",
    "        assert (\n",
    "            self.sample_softmax <= 0\n",
    "        ), \"Sampling from the softmax is not implemented yet. Please look at issue: #3310: https://github.com/huggingface/transformers/issues/3310\"\n",
    "\n",
    "        self.crit = TFAdaptiveSoftmaxMask(\n",
    "            config.vocab_size, config.d_embed, config.d_model, config.cutoffs, div_val=config.div_val, name=\"crit\"\n",
    "        )\n",
    "\n",
    "    def _resize_token_embeddings(self, new_num_tokens):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        \"\"\"Double-check if you are using adaptive softmax.\"\"\"\n",
    "        if len(self.crit.out_layers) > 0:\n",
    "            return self.crit.out_layers[-1]\n",
    "        return None\n",
    "\n",
    "    def reset_memory_length(self, mem_len):\n",
    "        self.transformer.reset_memory_length(mem_len)\n",
    "\n",
    "    def init_mems(self, bsz):\n",
    "        return self.transformer.init_mems(bsz)\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TFTransfoXLLMHeadModelOutput,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            mems=mems,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        if inputs[\"input_ids\"] is not None:\n",
    "            bsz, tgt_len = shape_list(inputs[\"input_ids\"])[:2]\n",
    "        else:\n",
    "            bsz, tgt_len = shape_list(inputs[\"inputs_embeds\"])[:2]\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs[\"input_ids\"],\n",
    "            inputs[\"mems\"],\n",
    "            inputs[\"head_mask\"],\n",
    "            inputs[\"inputs_embeds\"],\n",
    "            inputs[\"output_attentions\"],\n",
    "            inputs[\"output_hidden_states\"],\n",
    "            inputs[\"return_dict\"],\n",
    "            training=inputs[\"training\"],\n",
    "        )\n",
    "\n",
    "        last_hidden = transformer_outputs[0]\n",
    "        pred_hid = last_hidden[:, -tgt_len:]\n",
    "\n",
    "        softmax_output = self.crit(pred_hid, labels, training=inputs[\"training\"])\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            return (softmax_output,) + transformer_outputs[1:]\n",
    "\n",
    "        return TFTransfoXLLMHeadModelOutput(\n",
    "            prediction_scores=softmax_output,\n",
    "            mems=transformer_outputs.mems,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def serving_output(self, output):\n",
    "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\n",
    "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\n",
    "\n",
    "        return TFTransfoXLLMHeadModelOutput(\n",
    "            prediction_scores=output.prediction_scores,\n",
    "            mems=tf.convert_to_tensor(output.mems),\n",
    "            hidden_states=hs,\n",
    "            attentions=attns,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, inputs, past, **model_kwargs):\n",
    "        inputs = {\"input_ids\": inputs}\n",
    "\n",
    "        # if past is defined in model kwargs then use it for faster decoding\n",
    "        if past:\n",
    "            inputs[\"mems\"] = past\n",
    "\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "\n",
    "@add_start_docstrings(\n",
    "    \"\"\"\n",
    "    The Transfo XL Model transformer with a sequence classification head on top (linear layer).\n",
    "\n",
    "    [`TFTransfoXLForSequenceClassification`] uses the last token in order to do the classification,\n",
    "    as other causal models (e.g. GPT-1,GPT-2) do.\n",
    "\n",
    "    Since it does classification on the last token, it requires to know the position of the last token. If a\n",
    "    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each\n",
    "    row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot\n",
    "    guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take\n",
    "    the last value in each row of the batch).\n",
    "    \"\"\",\n",
    "    TRANSFO_XL_START_DOCSTRING,\n",
    ")\n",
    "class TFTransfoXLForSequenceClassification(TFTransfoXLPreTrainedModel, TFSequenceClassificationLoss):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.score = tf.keras.layers.Dense(\n",
    "            config.num_labels,\n",
    "            kernel_initializer=get_initializer(config.init_range),\n",
    "            name=\"score\",\n",
    "            use_bias=False,\n",
    "        )\n",
    "        self.transformer = TFTransfoXLMainLayer(config, name=\"transformer\")\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.transformer.word_emb\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)\n",
    "    @add_code_sample_docstrings(\n",
    "        processor_class=_TOKENIZER_FOR_DOC,\n",
    "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
    "        output_type=TFTransfoXLSequenceClassifierOutputWithPast,\n",
    "        config_class=_CONFIG_FOR_DOC,\n",
    "    )\n",
    "    def call(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mems=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        labels=None,\n",
    "        training=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the cross entropy classification loss. Indices should be in `[0, ..., config.vocab_size - 1]`.\n",
    "        \"\"\"\n",
    "        inputs = input_processing(\n",
    "            func=self.call,\n",
    "            config=self.config,\n",
    "            input_ids=input_ids,\n",
    "            mems=mems,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            labels=labels,\n",
    "            training=training,\n",
    "            kwargs_call=kwargs,\n",
    "        )\n",
    "\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            mems=inputs[\"mems\"],\n",
    "            head_mask=inputs[\"head_mask\"],\n",
    "            inputs_embeds=inputs[\"inputs_embeds\"],\n",
    "            output_attentions=inputs[\"output_attentions\"],\n",
    "            output_hidden_states=inputs[\"output_hidden_states\"],\n",
    "            return_dict=inputs[\"return_dict\"],\n",
    "            training=inputs[\"training\"],\n",
    "        )\n",
    "\n",
    "        hidden_states = transformer_outputs[0]\n",
    "        logits = self.score(hidden_states)\n",
    "        in_logits = None\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if inputs[\"input_ids\"] is not None:\n",
    "                sequence_lengths = (\n",
    "                    tf.reduce_sum(\n",
    "                        tf.cast(\n",
    "                            tf.math.not_equal(inputs[\"input_ids\"], self.config.pad_token_id),\n",
    "                            dtype=inputs[\"input_ids\"].dtype,\n",
    "                        ),\n",
    "                        -1,\n",
    "                        keepdims=False,\n",
    "                    )\n",
    "                    - 1\n",
    "                )\n",
    "                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "                logger.warning(\n",
    "                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n",
    "                    f\"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n",
    "                )\n",
    "        loss = None\n",
    "\n",
    "        if inputs[\"labels\"] is not None:\n",
    "            if input_ids is not None:\n",
    "                batch_size, sequence_length = shape_list(inputs[\"input_ids\"])[:2]\n",
    "            else:\n",
    "                batch_size, sequence_length = shape_list(inputs[\"inputs_embeds\"])[:2]\n",
    "            assert (\n",
    "                self.config.pad_token_id is not None or batch_size == 1\n",
    "            ), \"Cannot handle batch sizes > 1 if no padding token is defined.\"\n",
    "\n",
    "            if not tf.is_tensor(sequence_lengths):\n",
    "                in_logits = logits[0:batch_size, sequence_lengths]\n",
    "\n",
    "            loss = self.compute_loss(\n",
    "                tf.reshape(inputs[\"labels\"], [-1, 1]), tf.reshape(in_logits, [-1, self.num_labels])\n",
    "            )\n",
    "\n",
    "        pooled_logits = in_logits if in_logits is not None else logits\n",
    "\n",
    "        if not inputs[\"return_dict\"]:\n",
    "            output = (pooled_logits,) + transformer_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TFTransfoXLSequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            mems=transformer_outputs.mems,\n",
    "            hidden_states=transformer_outputs.hidden_states,\n",
    "            attentions=transformer_outputs.attentions,\n",
    "        ), self.config.num_labels\n",
    "\n",
    "    def serving_output(self, output):\n",
    "        hs = tf.convert_to_tensor(output.hidden_states) if self.config.output_hidden_states else None\n",
    "        attns = tf.convert_to_tensor(output.attentions) if self.config.output_attentions else None\n",
    "\n",
    "        return TFTransfoXLSequenceClassifierOutputWithPast(\n",
    "            logits=output.logits, mems=tf.convert_to_tensor(output.mems), hidden_states=hs, attentions=attns\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at transfo-xl-wt103 were not used when initializing TFTransfoXLForSequenceClassification: ['crit']\n",
      "- This IS expected if you are initializing TFTransfoXLForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFTransfoXLForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFTransfoXLForSequenceClassification were not initialized from the model checkpoint at transfo-xl-wt103 and are newly initialized: ['score']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = TFTransfoXLForSequenceClassification.from_pretrained(\n",
    "    \"transfo-xl-wt103\", num_labels=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.00025\n",
      "Data directory: /home/jun/workspace/wikitext-103\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ModelConfig:\n",
    "    def __init__(self):\n",
    "        # GPU config\n",
    "        self.num_core_per_host = 8\n",
    "\n",
    "        # Experiment (data/checkpoint/directory) config\n",
    "        self.data_dir = \"\"\n",
    "        self.record_info_dir = \"\"\n",
    "        self.corpus_info_path = \"/home/jun/workspace/wikitext-103/corpus-info.json\"\n",
    "        self.model_dir = None\n",
    "        self.do_train = True\n",
    "        self.do_eval = False\n",
    "        self.eval_ckpt_path = None\n",
    "        self.warm_start_path = None\n",
    "\n",
    "        # Optimization config\n",
    "        self.learning_rate = 2.5e-4\n",
    "        self.clip = 0.25\n",
    "        self.min_lr_ratio = 0.004\n",
    "        self.warmup_steps = 0\n",
    "\n",
    "        # Training config\n",
    "        self.train_batch_size = 60\n",
    "        self.eval_batch_size = 60\n",
    "        self.train_steps = 100000\n",
    "        self.iterations = 500\n",
    "        self.save_steps = 10000\n",
    "\n",
    "        # Evaluation config\n",
    "        self.do_test = False\n",
    "        self.max_eval_batch = -1\n",
    "        self.do_eval_only = False\n",
    "        self.start_eval_steps = 10000\n",
    "        self.eval_split = \"valid\"\n",
    "\n",
    "        # Model config\n",
    "        self.tgt_len = 70\n",
    "        self.mem_len = 70\n",
    "        self.same_length = False\n",
    "        self.clamp_len = -1\n",
    "        self.n_layer = 6\n",
    "        self.d_model = 500\n",
    "        self.d_embed = 500\n",
    "        self.n_head = 10\n",
    "        self.d_head = 50\n",
    "        self.d_inner = 1000\n",
    "        self.dropout = 0.1\n",
    "        self.dropatt = 0.1\n",
    "        self.untie_r = False\n",
    "\n",
    "        # Adaptive Softmax / Embedding\n",
    "        self.tie_weight = True\n",
    "        self.div_val = 1\n",
    "        self.proj_share_all_but_first = False\n",
    "        self.proj_same_dim = True\n",
    "\n",
    "        # Parameter initialization\n",
    "        self.init = \"normal\"\n",
    "        self.init_std = 0.02\n",
    "        self.proj_init_std = 0.01\n",
    "        self.init_range = 0.1\n",
    "\n",
    "class DataConfig:\n",
    "    def __init__(self):\n",
    "        # Data config\n",
    "        self.data_data_dir = '/home/jun/workspace/wikitext-103'\n",
    "        self.dataset = \"wt103\"\n",
    "        self.per_host_train_bsz = 60\n",
    "        self.per_host_valid_bsz = 60\n",
    "        self.per_host_test_bsz = 0\n",
    "        self.data_tgt_len = 70\n",
    "        self.max_batch = -1\n",
    "        self.data_num_core_per_host = 8\n",
    "        self.debug = False\n",
    "        self.num_procs = 1\n",
    "        self.num_passes = 10\n",
    "        self.num_shuffle = 4\n",
    "        self.use_tpu = True\n",
    "\n",
    "model_config = ModelConfig()\n",
    "data_config = DataConfig()\n",
    "\n",
    "# 예시: 설정 값 출력\n",
    "print(f\"Learning rate: {model_config.learning_rate}\")\n",
    "print(f\"Data directory: {data_config.data_data_dir}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 16:56:06.795805: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-17 16:56:07.333135: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-17 16:56:07.333178: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-17 16:56:07.333182: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict\n",
    "import math\n",
    "from functools import partial\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import logging\n",
    "# from gpu_utils import assign_to_gpu, average_grads_and_vars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "  def __init__(self, special=[], min_freq=0, max_size=None, lower_case=True,\n",
    "         delimiter=None, vocab_file=None):\n",
    "    self.counter = Counter()\n",
    "    self.special = special\n",
    "    self.min_freq = min_freq\n",
    "    self.max_size = max_size\n",
    "    self.lower_case = lower_case\n",
    "    self.delimiter = delimiter\n",
    "    self.vocab_file = vocab_file\n",
    "\n",
    "  def tokenize(self, line, add_eos=False, add_double_eos=False):\n",
    "    line = line.strip()\n",
    "    # convert to lower case\n",
    "    if self.lower_case:\n",
    "      line = line.lower()\n",
    "\n",
    "    # empty delimiter '' will evaluate False\n",
    "    if self.delimiter == '':\n",
    "      symbols = line\n",
    "    else:\n",
    "      symbols = line.split(self.delimiter)\n",
    "\n",
    "    if add_double_eos: # lm1b\n",
    "      return ['<S>'] + symbols + ['<S>']\n",
    "    elif add_eos:\n",
    "      return symbols + ['<eos>']\n",
    "    else:\n",
    "      return symbols\n",
    "\n",
    "  def count_file(self, path, verbose=False, add_eos=False):\n",
    "    if verbose: print('counting file {} ...'.format(path))\n",
    "    assert tf.io.gfile.exists(path)\n",
    "\n",
    "    sents = []\n",
    "    with open(path, 'r') as f:\n",
    "      for idx, line in enumerate(f):\n",
    "        if verbose and idx > 0 and idx % 500000 == 0:\n",
    "          print('  line {}'.format(idx))\n",
    "        symbols = self.tokenize(line, add_eos=add_eos)\n",
    "        self.counter.update(symbols)\n",
    "        sents.append(symbols)\n",
    "\n",
    "    return sents\n",
    "\n",
    "  def count_sents(self, sents, verbose=False):\n",
    "    \"\"\"\n",
    "      sents : a list of sentences, each a list of tokenized symbols\n",
    "    \"\"\"\n",
    "    if verbose: print('counting {} sents ...'.format(len(sents)))\n",
    "    for idx, symbols in enumerate(sents):\n",
    "      if verbose and idx > 0 and idx % 500000 == 0:\n",
    "        print('  line {}'.format(idx))\n",
    "      self.counter.update(symbols)\n",
    "\n",
    "  def _build_from_file(self, vocab_file):\n",
    "    self.idx2sym = []\n",
    "    self.sym2idx = OrderedDict()\n",
    "\n",
    "    with open(vocab_file, 'r') as f:\n",
    "      for line in f:\n",
    "        symb = line.strip().split()[0]\n",
    "        self.add_symbol(symb)\n",
    "    self.unk_idx = self.sym2idx['<UNK>']\n",
    "\n",
    "  def build_vocab(self):\n",
    "    if self.vocab_file:\n",
    "      print('building vocab from {}'.format(self.vocab_file))\n",
    "      self._build_from_file(self.vocab_file)\n",
    "      print('final vocab size {}'.format(len(self)))\n",
    "    else:\n",
    "      print('building vocab with min_freq={}, max_size={}'.format(\n",
    "        self.min_freq, self.max_size))\n",
    "      self.idx2sym = []\n",
    "      self.sym2idx = OrderedDict()\n",
    "\n",
    "      for sym in self.special:\n",
    "        self.add_special(sym)\n",
    "\n",
    "      for sym, cnt in self.counter.most_common(self.max_size):\n",
    "        if cnt < self.min_freq: break\n",
    "        self.add_symbol(sym)\n",
    "\n",
    "      print('final vocab size {} from {} unique tokens'.format(\n",
    "        len(self), len(self.counter)))\n",
    "\n",
    "  def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n",
    "          add_double_eos=False):\n",
    "    if verbose: print('encoding file {} ...'.format(path))\n",
    "    assert tf.io.gfile.exists(path)\n",
    "    encoded = []\n",
    "    with open(path, 'r') as f:\n",
    "      for idx, line in enumerate(f):\n",
    "        if verbose and idx > 0 and idx % 500000 == 0:\n",
    "          print('  line {}'.format(idx))\n",
    "        symbols = self.tokenize(line, add_eos=add_eos,\n",
    "          add_double_eos=add_double_eos)\n",
    "        encoded.append(self.convert_to_nparray(symbols))\n",
    "\n",
    "    if ordered:\n",
    "      encoded = np.concatenate(encoded)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "  def encode_sents(self, sents, ordered=False, verbose=False):\n",
    "    if verbose: print('encoding {} sents ...'.format(len(sents)))\n",
    "    encoded = []\n",
    "    for idx, symbols in enumerate(sents):\n",
    "      if verbose and idx > 0 and idx % 500000 == 0:\n",
    "        print('  line {}'.format(idx))\n",
    "      encoded.append(self.convert_to_nparray(symbols))\n",
    "\n",
    "    if ordered:\n",
    "      encoded = np.concatenate(encoded)\n",
    "\n",
    "    return encoded\n",
    "\n",
    "  def add_special(self, sym):\n",
    "    if sym not in self.sym2idx:\n",
    "      self.idx2sym.append(sym)\n",
    "      self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "      setattr(self, '{}_idx'.format(sym.strip('<>')), self.sym2idx[sym])\n",
    "\n",
    "  def add_symbol(self, sym):\n",
    "    if sym not in self.sym2idx:\n",
    "      self.idx2sym.append(sym)\n",
    "      self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "\n",
    "  def get_sym(self, idx):\n",
    "    assert 0 <= idx < len(self), 'Index {} out of range'.format(idx)\n",
    "    return self.idx2sym[idx]\n",
    "\n",
    "  def get_idx(self, sym):\n",
    "    if sym in self.sym2idx:\n",
    "      return self.sym2idx[sym]\n",
    "    else:\n",
    "      assert hasattr(self, 'unk_idx')\n",
    "      return self.sym2idx.get(sym, self.unk_idx)\n",
    "\n",
    "  def get_symbols(self, indices):\n",
    "    return [self.get_sym(idx) for idx in indices]\n",
    "\n",
    "  def get_indices(self, symbols):\n",
    "    return [self.get_idx(sym) for sym in symbols]\n",
    "\n",
    "  def convert_to_nparray(self, symbols):\n",
    "    nparray = np.array(self.get_indices(symbols), dtype=np.int64)\n",
    "    return nparray\n",
    "\n",
    "  def convert_to_sent(self, indices, exclude=None):\n",
    "    if exclude is None:\n",
    "      return ' '.join([self.get_sym(idx) for idx in indices])\n",
    "    else:\n",
    "      return ' '.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.idx2sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _preprocess(shard, train, vocab, save_dir, cutoffs, bin_sizes, bsz, tgt_len,\n",
    "                num_core_per_host, use_tpu, num_shuffle):\n",
    "  file_names = []\n",
    "  num_batch = 0\n",
    "\n",
    "  path = train[shard]\n",
    "  data_shard = vocab.encode_file(path, ordered=False, add_double_eos=True)\n",
    "\n",
    "  for shuffle in range(num_shuffle):\n",
    "    basename = \"train-{:03d}-{:02d}\".format(shard, shuffle)\n",
    "    print(\"Processing shard {} shuffle {}\".format(shard, shuffle))\n",
    "\n",
    "    np.random.shuffle(data_shard)\n",
    "    file_name, num_batch_shuffle = create_ordered_tfrecords(\n",
    "        save_dir, basename, np.concatenate(data_shard), bsz, tgt_len,\n",
    "        num_core_per_host, cutoffs, bin_sizes, use_tpu=use_tpu)\n",
    "    file_names.append(file_name)\n",
    "    num_batch += num_batch_shuffle\n",
    "\n",
    "  return file_names, num_batch\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "  def __init__(self, path, dataset, *args, **kwargs):\n",
    "    self.dataset = dataset\n",
    "    self.vocab = Vocab(*args, **kwargs)\n",
    "\n",
    "    if self.dataset in [\"ptb\", \"wt2\", \"enwik8\", \"text8\"]:\n",
    "      self.vocab.count_file(os.path.join(path, \"train.txt\"))\n",
    "      self.vocab.count_file(os.path.join(path, \"valid.txt\"))\n",
    "      self.vocab.count_file(os.path.join(path, \"test.txt\"))\n",
    "    elif self.dataset == \"wt103\":\n",
    "      self.vocab.count_file(os.path.join(path, \"train.txt\"))\n",
    "    elif self.dataset == \"lm1b\":\n",
    "      train_path_pattern = os.path.join(\n",
    "          path, \"1-billion-word-language-modeling-benchmark-r13output\",\n",
    "          \"training-monolingual.tokenized.shuffled\", \"news.en-*\")\n",
    "      train_paths = tf.io.gfile.glob(train_path_pattern)\n",
    "\n",
    "      # the vocab will load from file when build_vocab() is called\n",
    "      # for train_path in sorted(train_paths):\n",
    "      #   self.vocab.count_file(train_path, verbose=True)\n",
    "\n",
    "    self.vocab.build_vocab()\n",
    "\n",
    "    if self.dataset in [\"ptb\", \"wt2\", \"wt103\"]:\n",
    "      self.train = self.vocab.encode_file(\n",
    "          os.path.join(path, \"train.txt\"), ordered=True)\n",
    "      self.valid = self.vocab.encode_file(\n",
    "          os.path.join(path, \"valid.txt\"), ordered=True)\n",
    "      self.test  = self.vocab.encode_file(\n",
    "          os.path.join(path, \"test.txt\"), ordered=True)\n",
    "    elif self.dataset in [\"enwik8\", \"text8\"]:\n",
    "      self.train = self.vocab.encode_file(\n",
    "          os.path.join(path, \"train.txt\"), ordered=True, add_eos=False)\n",
    "      self.valid = self.vocab.encode_file(\n",
    "          os.path.join(path, \"valid.txt\"), ordered=True, add_eos=False)\n",
    "      self.test  = self.vocab.encode_file(\n",
    "          os.path.join(path, \"test.txt\"), ordered=True, add_eos=False)\n",
    "    elif self.dataset == \"lm1b\":\n",
    "      self.train = train_paths\n",
    "      valid_path = os.path.join(path, \"valid.txt\")\n",
    "      test_path = valid_path\n",
    "      self.valid = self.vocab.encode_file(\n",
    "          valid_path, ordered=True, add_double_eos=True)\n",
    "      self.test  = self.vocab.encode_file(\n",
    "          test_path, ordered=True, add_double_eos=True)\n",
    "\n",
    "    if self.dataset == \"wt103\":\n",
    "      self.cutoffs = [0, 20000, 40000, 200000] + [len(self.vocab)]\n",
    "    elif self.dataset == \"lm1b\":\n",
    "      self.cutoffs = [0, 60000, 100000, 640000] + [len(self.vocab)]\n",
    "    else:\n",
    "      self.cutoffs = []\n",
    "\n",
    "\n",
    "  def convert_to_tfrecords(self, split, save_dir, bsz, tgt_len,\n",
    "                           num_core_per_host, config): #**kwargs):\n",
    "    data_config = config\n",
    "\n",
    "    file_names = []\n",
    "    use_tpu = data_config.use_tpu and not (split == \"test\" and num_core_per_host == 1)\n",
    "\n",
    "    if use_tpu:\n",
    "      record_name = \"record_info-{}.bsz-{}.tlen-{}.core-{}.json\".format(\n",
    "           split, bsz, tgt_len, num_core_per_host)\n",
    "    else:\n",
    "      record_name = \"record_info-{}.bsz-{}.tlen-{}.json\".format(\n",
    "           split, bsz, tgt_len)\n",
    "\n",
    "    record_info_path = os.path.join(save_dir, record_name)\n",
    "\n",
    "    if self.dataset in [\"ptb\", \"wt2\", \"wt103\", \"enwik8\", \"text8\"]:\n",
    "      data = getattr(self, split)\n",
    "      bin_sizes = get_bin_sizes(\n",
    "          data, bsz // num_core_per_host, tgt_len, self.cutoffs)\n",
    "      file_name, num_batch = create_ordered_tfrecords(\n",
    "          save_dir, split, data, bsz, tgt_len, num_core_per_host,\n",
    "          self.cutoffs, bin_sizes,\n",
    "          num_passes=data_config.num_passes if split == 'train' and use_tpu else 1,\n",
    "          use_tpu=use_tpu)\n",
    "      file_names.append(file_name)\n",
    "    elif self.dataset == \"lm1b\":\n",
    "      bin_sizes = get_bin_sizes(\n",
    "          self.valid, bsz // num_core_per_host, tgt_len, self.cutoffs)\n",
    "      if split == \"train\":\n",
    "        np.random.seed(123456)\n",
    "        num_batch = 0\n",
    "\n",
    "        if data_config.num_procs > 1:\n",
    "          _preprocess_wrapper = partial(_preprocess,\n",
    "              train=self.train, vocab=self.vocab, save_dir=save_dir,\n",
    "              cutoffs=self.cutoffs, bin_sizes=bin_sizes, bsz=bsz,\n",
    "              tgt_len=tgt_len, num_core_per_host=num_core_per_host,\n",
    "              use_tpu=use_tpu, num_shuffle=data_config.num_shuffle)\n",
    "\n",
    "          pool = mp.Pool(processes=data_config.num_procs)\n",
    "          results = pool.map(_preprocess_wrapper, range(len(self.train)))\n",
    "          for res in results:\n",
    "            file_names.extend(res[0])\n",
    "            num_batch += res[1]\n",
    "        else:\n",
    "          for shard, path in enumerate(self.train):\n",
    "            data_shard = self.vocab.encode_file(path, ordered=False,\n",
    "                                                add_double_eos=True)\n",
    "\n",
    "            num_shuffle = data_config.num_shuffle\n",
    "\n",
    "            for shuffle in range(num_shuffle):\n",
    "              print(\"Processing shard {} shuffle {}\".format(shard, shuffle))\n",
    "              basename = \"train-{:03d}-{:02d}\".format(shard, shuffle)\n",
    "              np.random.shuffle(data_shard)\n",
    "              file_name, num_batch_ = create_ordered_tfrecords(\n",
    "                  save_dir, basename, np.concatenate(data_shard), bsz, tgt_len,\n",
    "                  num_core_per_host,\n",
    "                  self.cutoffs, bin_sizes, use_tpu=use_tpu)\n",
    "              file_names.append(file_name)\n",
    "              num_batch += num_batch_\n",
    "\n",
    "      else:\n",
    "        file_name, num_batch = create_ordered_tfrecords(\n",
    "            save_dir, split, getattr(self, split), bsz, tgt_len,\n",
    "            num_core_per_host,\n",
    "            self.cutoffs, bin_sizes, use_tpu=use_tpu)\n",
    "        file_names.append(file_name)\n",
    "\n",
    "    with open(record_info_path, \"w\") as fp:\n",
    "      record_info = {\n",
    "        \"filenames\": file_names,\n",
    "        \"bin_sizes\": bin_sizes,\n",
    "        \"num_batch\": num_batch\n",
    "      }\n",
    "      json.dump(record_info, fp)\n",
    "\n",
    "\n",
    "def get_bin_sizes(data, batch_size, tgt_len, cutoffs, std_mult=[2.5, 2.5, 2.5]):\n",
    "  \"\"\"\n",
    "    Note: the `batch_size` here should be per-core batch size\n",
    "  \"\"\"\n",
    "  bin_sizes = []\n",
    "\n",
    "  def _nearest_to_eight(x): # so that it's faster on TPUs\n",
    "    y = x - x % 8\n",
    "    return y + 8 if x % 8 >= 4 else max(8, y)\n",
    "\n",
    "  if cutoffs:\n",
    "    num_batch = len(data) // batch_size // tgt_len\n",
    "\n",
    "    data = data[:batch_size * num_batch * tgt_len]\n",
    "    data = data.reshape(batch_size, num_batch, tgt_len)\n",
    "\n",
    "    tot = batch_size * tgt_len\n",
    "    for b, (left, right) in enumerate(zip(cutoffs[1:-1], cutoffs[2:])):\n",
    "      mask = (data >= left) * (data < right)\n",
    "      percents = mask.astype(np.float64).sum(2).sum(0) / tot\n",
    "      mean = np.mean(percents)\n",
    "      std = np.std(percents)\n",
    "\n",
    "      bin_size = int(math.ceil(tgt_len * batch_size * (mean + std_mult[b] * std)))\n",
    "      bin_size = _nearest_to_eight(bin_size)\n",
    "      bin_sizes.append(bin_size)\n",
    "\n",
    "  return bin_sizes\n",
    "\n",
    "\n",
    "def _int64_feature(values):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=values))\n",
    "\n",
    "def _float_feature(values):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=values))\n",
    "\n",
    "def batchify(data, batch_size, num_passes):\n",
    "  \"\"\"\n",
    "    if use_tpu = True: num_passes > 1 \n",
    "    \n",
    "    Since TPU training requires entire [bsz x tgt_len] chunks, it can discard\n",
    "    as many as `bsz * tgt_len` tokens in training. When `bsz` and `tgt_len` are \n",
    "    both large, as in the case of TPU training for Transformer-XL, the problem\n",
    "    may lead to detectable performance drop. \n",
    "\n",
    "    Here, we use multiple randomly shifted copies to deal with this problem.\n",
    "  \"\"\"\n",
    "  if num_passes > 1:\n",
    "    data_len = len(data)\n",
    "    double_data = np.concatenate([data, data])\n",
    "    data_list = []\n",
    "    for i in range(num_passes):\n",
    "      start = np.random.randint(0, data_len)\n",
    "      data_list.append(double_data[start:start+data_len])\n",
    "    data = np.concatenate(data_list)\n",
    "\n",
    "  num_step = len(data) // batch_size\n",
    "  data = data[:batch_size * num_step]\n",
    "  data = data.reshape(batch_size, num_step)\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def create_ordered_tfrecords(save_dir, basename, data, batch_size, tgt_len,\n",
    "                             num_core_per_host, cutoffs=[], bin_sizes=[], \n",
    "                             num_passes=1, use_tpu=False):\n",
    "\n",
    "  if use_tpu:\n",
    "    file_name = \"{}.bsz-{}.tlen-{}.core-{}.tfrecords\".format(\n",
    "        basename, batch_size, tgt_len, num_core_per_host)\n",
    "  else:\n",
    "    file_name = \"{}.bsz-{}.tlen-{}.tfrecords\".format(\n",
    "        basename, batch_size, tgt_len)\n",
    "\n",
    "  save_path = os.path.join(save_dir, file_name)\n",
    "  record_writer = tf.io.TFRecordWriter(save_path)\n",
    "\n",
    "  batched_data = batchify(data, batch_size, num_passes)\n",
    "\n",
    "  num_batch = 0\n",
    "  # for t in range(0, batched_data.shape[1] - tgt_len - 1, tgt_len):\n",
    "  for t in range(0, batched_data.shape[1] - 1, tgt_len):\n",
    "    cur_tgt_len = min(batched_data.shape[1] - 1 - t, tgt_len)\n",
    "    # drop the remainder if use tpu\n",
    "    if use_tpu and cur_tgt_len < tgt_len: \n",
    "      break\n",
    "    if num_batch % 500 == 0:\n",
    "      print(\"  processing batch {}\".format(num_batch))\n",
    "    for idx in range(batch_size):\n",
    "      inputs = batched_data[idx, t:t + cur_tgt_len]\n",
    "      labels = batched_data[idx, t + 1:t + cur_tgt_len + 1]\n",
    "\n",
    "      # features dict\n",
    "      feature = {\n",
    "          \"inputs\": _int64_feature(inputs),\n",
    "          \"labels\": _int64_feature(labels),\n",
    "      }\n",
    "\n",
    "      if len(cutoffs) > 0 and use_tpu:\n",
    "        # validate `bin_sizes` and `cutoffs`\n",
    "        assert len(cutoffs) - len(bin_sizes) == 2, \\\n",
    "          \"len(cutoffs) - len(bin_sizes) != 2\"\n",
    "\n",
    "        # mask for bin 0\n",
    "        left, right = cutoffs[:2]\n",
    "        inp_mask = ((inputs >= left) * (inputs < right)).astype(np.float32)\n",
    "        tgt_mask = ((labels >= left) * (labels < right)).astype(np.float32)\n",
    "\n",
    "        feature[\"inp_mask\"] = _float_feature(inp_mask)\n",
    "        feature[\"tgt_mask\"] = _float_feature(tgt_mask)\n",
    "\n",
    "        # refresh `inp_cnts` and `tgt_cnts` for each TPU core\n",
    "        if idx % (batch_size // num_core_per_host) == 0:\n",
    "          inp_cnts = [0] * len(bin_sizes)\n",
    "          tgt_cnts = [0] * len(bin_sizes)\n",
    "\n",
    "        head_labels = np.copy(labels)\n",
    "        inp_pos_per_bin, tgt_pos_per_bin = [], []\n",
    "        for b, (left, right) in enumerate(zip(cutoffs[1:-1], cutoffs[2:])):\n",
    "          inp_pos = np.where((inputs >= left) * (inputs < right))[0]\n",
    "          tgt_pos = np.where((labels >= left) * (labels < right))[0]\n",
    "          inp_pos_per_bin.append(inp_pos)\n",
    "          tgt_pos_per_bin.append(tgt_pos)\n",
    "\n",
    "          head_labels[tgt_pos] = cutoffs[1] + b\n",
    "\n",
    "        feature[\"head_labels\"] = _int64_feature(head_labels)\n",
    "\n",
    "        # permutation feature\n",
    "        def _add_perm_feature(feature, pos_per_bin, cnts, prefix):\n",
    "          for b, pos in enumerate(pos_per_bin):\n",
    "            idx_tuple = []\n",
    "            for p in pos:\n",
    "              if cnts[b] < bin_sizes[b]:\n",
    "                idx_tuple.append([p, cnts[b]])\n",
    "                cnts[b] += 1\n",
    "              else:\n",
    "                break\n",
    "\n",
    "            n_tup = len(idx_tuple)\n",
    "            tup = np.array(idx_tuple).reshape(n_tup * 2)\n",
    "\n",
    "            feature[\"{}_cnt_{}\".format(prefix, b)] = _int64_feature([n_tup])\n",
    "            feature[\"{}_tup_{}\".format(prefix, b)] = _int64_feature(tup)\n",
    "\n",
    "        _add_perm_feature(feature, inp_pos_per_bin, inp_cnts, \"inp\")\n",
    "        _add_perm_feature(feature, tgt_pos_per_bin, tgt_cnts, \"tgt\")\n",
    "\n",
    "      example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "      record_writer.write(example.SerializeToString())\n",
    "\n",
    "    num_batch += 1\n",
    "\n",
    "  record_writer.close()\n",
    "  print(\"Done writing {}. batches: {}\".format(file_name, num_batch))\n",
    "\n",
    "  return file_name, num_batch\n",
    "\n",
    "\n",
    "def get_lm_corpus(data_dir, dataset):\n",
    "  fn = os.path.join(data_dir, \"cache.pkl\")\n",
    "\n",
    "  if tf.io.gfile.exists(fn):\n",
    "    print(\"Loading cached dataset...\")\n",
    "    with open(fn, \"rb\") as fp:\n",
    "      corpus = pickle.load(fp)\n",
    "  else:\n",
    "    print(\"Producing dataset...\")\n",
    "    kwargs = {}\n",
    "    if dataset in [\"wt103\", \"wt2\"]:\n",
    "      kwargs[\"special\"] = [\"<eos>\"]\n",
    "      kwargs[\"lower_case\"] = False\n",
    "    elif dataset == \"ptb\":\n",
    "      kwargs[\"special\"] = [\"<eos>\"]\n",
    "      kwargs[\"lower_case\"] = True\n",
    "    elif dataset == \"lm1b\":\n",
    "      kwargs[\"special\"] = []\n",
    "      kwargs[\"lower_case\"] = False\n",
    "      kwargs[\"vocab_file\"] = os.path.join(data_dir, \"1b_word_vocab.txt\")\n",
    "    elif dataset in [\"enwik8\", \"text8\"]:\n",
    "      pass\n",
    "\n",
    "    corpus = Corpus(data_dir, dataset, **kwargs)\n",
    "\n",
    "    print(\"Saving dataset...\")\n",
    "    with open(fn, \"wb\") as fp:\n",
    "      pickle.dump(corpus, fp, protocol=2)\n",
    "\n",
    "    corpus_info = {\n",
    "      \"vocab_size\" : len(corpus.vocab),\n",
    "      \"cutoffs\" : corpus.cutoffs,\n",
    "      \"dataset\" : corpus.dataset\n",
    "    }\n",
    "    with open(os.path.join(data_dir, \"corpus-info.json\"), \"w\") as fp:\n",
    "      json.dump(corpus_info, fp)\n",
    "\n",
    "  return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = ModelConfig()\n",
    "data_config = DataConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached dataset...\n"
     ]
    }
   ],
   "source": [
    "corpus = get_lm_corpus(data_config.data_data_dir, data_config.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(data_config.data_data_dir, \"tfrecords\")\n",
    "if not tf.io.gfile.exists(save_dir):\n",
    "    tf.io.gfile.makedirs(save_dir)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    " # test mode\n",
    "# if data_config.per_host_test_bsz > 0:\n",
    "#     corpus.convert_to_tfrecords(\"test\", save_dir, data_config.per_host_test_bsz,\n",
    "#                                 data_config.per_host_test_bsz, data_config.data_num_core_per_host, \n",
    "#                                 FLAGS=data_config)\n",
    "\n",
    "# for split, batch_size in zip(\n",
    "#     [\"train\", \"valid\"],\n",
    "#     [data_config.per_host_train_bsz, data_config.per_host_valid_bsz]):\n",
    "\n",
    "#     if batch_size <= 0: continue\n",
    "#     print(\"Converting {} set...\".format(split))\n",
    "#     corpus.convert_to_tfrecords(split, save_dir, batch_size, data_config.data_tgt_len,\n",
    "#                                 data_config.data_num_core_per_host, FLAGS=data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_info(corpus_info_path):\n",
    "  with open(corpus_info_path, \"r\") as fp:\n",
    "    corpus_info = json.load(fp)\n",
    "  return corpus_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_record_info(record_info_dir, split, per_host_bsz, tgt_len,\n",
    "                     num_core_per_host, use_tpu):\n",
    "  if use_tpu:\n",
    "    record_name = \"record_info-{}.bsz-{}.tlen-{}.core-{}.json\".format(\n",
    "        split, per_host_bsz, tgt_len, num_core_per_host)\n",
    "  else:\n",
    "    record_name = \"record_info-{}.bsz-{}.tlen-{}.json\".format(\n",
    "        split, per_host_bsz, tgt_len)\n",
    "\n",
    "  record_info_path = os.path.join(record_info_dir, record_name)\n",
    "  with open(record_info_path, \"r\") as fp:\n",
    "    record_info = json.load(fp)\n",
    "\n",
    "  return record_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_fn(record_info_dir, split, per_host_bsz, tgt_len,\n",
    "                 num_core_per_host, num_hosts=1, use_tpu=False):\n",
    "  \"\"\"Creates input function.\"\"\"\n",
    "  record_info = load_record_info(record_info_dir, split, per_host_bsz, tgt_len,\n",
    "                                 num_core_per_host, use_tpu=use_tpu)\n",
    "\n",
    "  file_names = record_info[\"filenames\"]\n",
    "  bin_sizes = record_info[\"bin_sizes\"]\n",
    "  num_batch = record_info[\"num_batch\"]\n",
    "\n",
    "  logging.info(\"[{}] File names {}\".format(split, file_names))\n",
    "\n",
    "  def input_fn(params):\n",
    "    # per-core batch size\n",
    "    per_core_bsz = params[\"batch_size\"]\n",
    "\n",
    "    # data_dir could be a remote path, e.g., a google storage url\n",
    "    data_dir = params[\"data_dir\"]\n",
    "\n",
    "    def parser(record):\n",
    "      # preprocess \"inp_perm\" and \"tgt_perm\"\n",
    "      def _process_perm_feature(example, prefix):\n",
    "        for b in range(len(bin_sizes)):\n",
    "          cnt = example.pop(\"{}_cnt_{}\".format(prefix, b))[0]\n",
    "          tup = example.pop(\"{}_tup_{}\".format(prefix, b))\n",
    "\n",
    "          tup = tf.reshape(\n",
    "              tf.sparse.to_dense(tup),\n",
    "              shape=[cnt, 2])\n",
    "\n",
    "          # tf.float32\n",
    "          perm = tf.sparse.to_dense(\n",
    "              sparse_indices=tup,\n",
    "              output_shape=[tgt_len, bin_sizes[b]],\n",
    "              sparse_values=1.0,\n",
    "              default_value=0.0)\n",
    "\n",
    "          example[\"{}_perm_{}\".format(prefix, b)] = perm\n",
    "\n",
    "      # whether allow the last batch with a potentially shorter length\n",
    "      if use_tpu:\n",
    "        record_spec = {\n",
    "            \"inputs\": tf.io.FixedLenFeature([tgt_len], tf.int64),\n",
    "            \"labels\": tf.io.FixedLenFeature([tgt_len], tf.int64),\n",
    "        }\n",
    "      else:\n",
    "        record_spec = {\n",
    "            \"inputs\": tf.io.VarLenFeature(tf.int64),\n",
    "            \"labels\": tf.io.VarLenFeature(tf.int64),\n",
    "        }\n",
    "\n",
    "      # permutation related features\n",
    "      if bin_sizes and use_tpu:\n",
    "        # tf.float32\n",
    "        record_spec[\"inp_mask\"] = tf.io.FixedLenFeature([tgt_len], tf.float32)\n",
    "        record_spec[\"tgt_mask\"] = tf.io.FixedLenFeature([tgt_len], tf.float32)\n",
    "\n",
    "        record_spec[\"head_labels\"] = tf.io.FixedLenFeature([tgt_len], tf.int64)\n",
    "\n",
    "        for b in range(len(bin_sizes)):\n",
    "          record_spec[\"inp_cnt_{}\".format(b)] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "          record_spec[\"inp_tup_{}\".format(b)] = tf.io.VarLenFeature(tf.int64)\n",
    "          record_spec[\"tgt_cnt_{}\".format(b)] = tf.io.FixedLenFeature([1], tf.int64)\n",
    "          record_spec[\"tgt_tup_{}\".format(b)] = tf.io.VarLenFeature(tf.int64)\n",
    "\n",
    "      # retrieve serialized example\n",
    "      example = tf.parse_single_example(\n",
    "          serialized=record,\n",
    "          features=record_spec)\n",
    "\n",
    "      # transform permutation tuples to permutation matrices\n",
    "      if bin_sizes and use_tpu:\n",
    "        _process_perm_feature(example, \"inp\")\n",
    "        _process_perm_feature(example, \"tgt\")\n",
    "\n",
    "      # cast int64 into int32\n",
    "      # cast sparse to dense\n",
    "      for key in list(example.keys()):\n",
    "        val = example[key]\n",
    "        if tf.keras.backend.is_sparse(val):\n",
    "          val = tf.sparse.to_dense(val)\n",
    "        if val.dtype == tf.int64:\n",
    "          val = tf.to_int32(val)\n",
    "        example[key] = val\n",
    "\n",
    "      if use_tpu:\n",
    "        return example\n",
    "      else:\n",
    "        return example[\"inputs\"], example[\"labels\"]\n",
    "\n",
    "    file_paths = []\n",
    "    for file_name in file_names:\n",
    "      file_path = os.path.join(data_dir, file_name)\n",
    "      file_paths.append(file_path)\n",
    "\n",
    "    if split == \"train\":\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "      if len(file_paths) > 1:\n",
    "        dataset = dataset.shuffle(len(file_paths)).repeat()\n",
    "        dataset = tf.data.TFRecordDataset(dataset)\n",
    "      elif num_hosts > 1:\n",
    "        host_id = params[\"context\"].current_host\n",
    "        # drop the remaining batches\n",
    "        num_batch_per_host = num_batch // num_hosts\n",
    "\n",
    "        my_start_sample_id = (host_id * num_batch_per_host * num_core_per_host *\n",
    "                              per_core_bsz)\n",
    "        my_sample_num = num_batch_per_host * num_core_per_host * per_core_bsz\n",
    "        dataset = tf.data.TFRecordDataset(dataset).skip(\n",
    "            my_start_sample_id).take(my_sample_num)\n",
    "      else:\n",
    "        dataset = tf.data.TFRecordDataset(dataset)\n",
    "\n",
    "      dataset = dataset.map(parser).cache().repeat()\n",
    "      dataset = dataset.batch(per_core_bsz, drop_remainder=True)\n",
    "      dataset = dataset.prefetch(num_core_per_host * per_core_bsz)\n",
    "    else:\n",
    "      # do not shuffle, repeat or cache in evaluation\n",
    "      dataset = tf.data.Dataset.from_tensor_slices(file_paths)\n",
    "      dataset = tf.data.TFRecordDataset(dataset)\n",
    "      dataset = dataset.map(parser)\n",
    "      dataset = dataset.batch(per_core_bsz, drop_remainder=True)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "  if split == \"train\" and num_hosts > 1:\n",
    "    record_info[\"num_batch\"] = num_batch // num_hosts\n",
    "\n",
    "  return input_fn, record_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jun/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 17:02:44.758991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.759309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.763511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.763806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.764087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.764366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.765273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-17 17:02:44.913789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.914084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.914344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.914590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.914836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:44.915081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.495052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.495364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.495627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.495878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.496123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.496362: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-17 17:02:45.496630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-17 17:02:45.496861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n",
      "WARNING:tensorflow:From /home/jun/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_112360/77423484.py\", line 67, in parser  *\n        example = tf.parse_single_example(\n\n    AttributeError: module 'tensorflow' has no attribute 'parse_single_example'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_112360/754847897.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m train_set = train_input_fn({\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \"data_dir\": model_config.data_dir})\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_112360/77423484.py\u001b[0m in \u001b[0;36minput_fn\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_core_bsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m       \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_core_per_host\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mper_core_bsz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2292\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[1;32m   2293\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[0;32m-> 2294\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2295\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2296\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5502\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5503\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   5504\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5505\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \"\"\"\n\u001b[1;32m    226\u001b[0m     concrete_function = self._get_concrete_function_garbage_collected(\n\u001b[0;32m--> 227\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m       \u001b[0mconcrete_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_concrete_function_internal_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneralized_func_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_placeholder_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m           \u001b[0mconcrete_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m           \u001b[0mgraph_capture_container\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcrete_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_capture_func_lib\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mautograph_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_autograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mspec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1283\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    238\u001b[0m           attributes=defun_kwargs)\n\u001b[1;32m    239\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=missing-docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36mwrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_should_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_should_pack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m           \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/__autograph_generated_filesdx9ljxt.py\u001b[0m in \u001b[0;36mtf__parser\u001b[0;34m(record)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mif_stmt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mand_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_tpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mif_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melse_body_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_state_3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"record_spec['head_labels']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"record_spec['inp_mask']\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"record_spec['tgt_mask']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_single_example\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mget_state_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_112360/77423484.py\", line 67, in parser  *\n        example = tf.parse_single_example(\n\n    AttributeError: module 'tensorflow' has no attribute 'parse_single_example'\n"
     ]
    }
   ],
   "source": [
    "train_input_fn, train_record_info = get_input_fn(\n",
    "      record_info_dir='/home/jun/workspace/wikitext-103/tfrecords',\n",
    "      split=\"train\",\n",
    "      per_host_bsz=model_config.train_batch_size,\n",
    "      tgt_len=model_config.tgt_len,\n",
    "      num_core_per_host=model_config.num_core_per_host,\n",
    "      num_hosts=1,\n",
    "      use_tpu=False)\n",
    "\n",
    "logging.info(\"num of batches {}\".format(train_record_info[\"num_batch\"]))\n",
    "\n",
    "##### Create computational graph\n",
    "train_set = train_input_fn({\n",
    "    \"batch_size\": model_config.train_batch_size,\n",
    "    \"data_dir\": model_config.data_dir})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.record_info_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_core_graph(n_token, cutoffs, is_training, inp, tgt, mems):\n",
    "  model_fn = get_model_fn(\n",
    "      n_token=n_token,\n",
    "      cutoffs=cutoffs)\n",
    "\n",
    "  model_ret = model_fn(\n",
    "      inp=inp,\n",
    "      tgt=tgt,\n",
    "      mems=mems,\n",
    "      is_training=is_training)\n",
    "\n",
    "  return model_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_token, cutoffs, ps_device):\n",
    "  ##### Get input function and model function\n",
    "  train_input_fn, train_record_info = get_input_fn(\n",
    "      record_info_dir=model_config.record_info_dir,\n",
    "      split=\"train\",\n",
    "      per_host_bsz=model_config.train_batch_size,\n",
    "      tgt_len=model_config.tgt_len,\n",
    "      num_core_per_host=model_config.num_core_per_host,\n",
    "      num_hosts=1,\n",
    "      use_tpu=False)\n",
    "\n",
    "  logging.info(\"num of batches {}\".format(train_record_info[\"num_batch\"]))\n",
    "\n",
    "  ##### Create computational graph\n",
    "  train_set = train_input_fn({\n",
    "      \"batch_size\": model_config.train_batch_size,\n",
    "      \"data_dir\": model_config.data_dir})\n",
    "\n",
    "  input_feed, label_feed = next(iter(train_set))\n",
    "\n",
    "  inputs = tf.split(input_feed, model_config.num_core_per_host, 0)\n",
    "  labels = tf.split(label_feed, model_config.num_core_per_host, 0)\n",
    "\n",
    "  per_core_bsz = model_config.train_batch_size // model_config.num_core_per_host\n",
    "\n",
    "  tower_mems, tower_losses, tower_new_mems, tower_grads_and_vars = [], [], [], []\n",
    "\n",
    "  for i in range(model_config.num_core_per_host):\n",
    "    with tf.device(assign_to_gpu(i, ps_device)):\n",
    "      mems_i = [tf.zeros([model_config.mem_len, per_core_bsz, model_config.d_model], dtype=tf.float32) for _ in range(model_config.n_layer)]\n",
    "\n",
    "\n",
    "      loss_i, new_mems_i, grads_and_vars_i = single_core_graph(\n",
    "          n_token=n_token,\n",
    "          cutoffs=cutoffs,\n",
    "          is_training=True,\n",
    "          inp=inputs[i],\n",
    "          tgt=labels[i],\n",
    "          mems=mems_i)\n",
    "\n",
    "      tower_mems.append(mems_i)\n",
    "      tower_losses.append(loss_i)\n",
    "      tower_new_mems.append(new_mems_i)\n",
    "      tower_grads_and_vars.append(grads_and_vars_i)\n",
    "\n",
    "  ## average losses and gradients across towers\n",
    "  if len(tower_losses) > 1:\n",
    "    # loss = tf.add_n(tower_losses) / len(tower_losses)\n",
    "    loss = sum(tower_losses) / len(tower_losses)\n",
    "    grads_and_vars = average_grads_and_vars(tower_grads_and_vars)\n",
    "  else:\n",
    "    loss = tower_losses[0]\n",
    "    grads_and_vars = tower_grads_and_vars[0]\n",
    "  grads, all_vars = zip(*grads_and_vars)\n",
    "\n",
    "  ## clip gradient -> gradient explosion) 문제를 방지하기 위한 기술\n",
    "  clipped, gnorm = tf.clip_by_global_norm(grads, model_config.clip)\n",
    "  grads_and_vars = list(zip(clipped, all_vars))\n",
    "\n",
    "  ## configure the optimizer\n",
    "  global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "  # warmup stage: increase the learning rate linearly\n",
    "  if model_config.warmup_steps > 0:\n",
    "    warmup_lr = tf.to_float(global_step) / tf.to_float(model_config.warmup_steps) \\\n",
    "                * model_config.learning_rate\n",
    "  else:\n",
    "    warmup_lr = 0.0\n",
    "\n",
    "  # decay stage: decay the learning rate using the cosine schedule\n",
    "  decay_lr = tf.train.cosine_decay(\n",
    "      model_config.learning_rate,\n",
    "      global_step=global_step-model_config.warmup_steps,\n",
    "      decay_steps=model_config.train_steps-model_config.warmup_steps,\n",
    "      alpha=model_config.min_lr_ratio)\n",
    "\n",
    "  # choose warmup or decay\n",
    "  learning_rate = tf.where(global_step < model_config.warmup_steps,\n",
    "                           warmup_lr, decay_lr)\n",
    "\n",
    "  # get the train op\n",
    "  optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "  train_op = optimizer.apply_gradients(grads_and_vars, global_step)\n",
    "\n",
    "  ##### Training loop\n",
    "  tower_mems_np = [\n",
    "      [np.zeros([model_config.mem_len, per_core_bsz, model_config.d_model], dtype=np.float32)\n",
    "          for layer in range(model_config.n_layer)]\n",
    "      for core in range(model_config.num_core_per_host)\n",
    "  ]\n",
    "\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "  with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    if model_config.warm_start_path is not None:\n",
    "      logging.info(\"warm start from {}\".format(model_config.warm_start_path))\n",
    "      saver.restore(sess, model_config.warm_start_path)\n",
    "\n",
    "    fetches = [loss, tower_new_mems, global_step, gnorm, learning_rate, train_op]\n",
    "\n",
    "    total_loss, prev_step = 0., -1\n",
    "    while True:\n",
    "      feed_dict = {}\n",
    "      for i in range(model_config.num_core_per_host):\n",
    "        for m, m_np in zip(tower_mems[i], tower_mems_np[i]):\n",
    "          feed_dict[m] = m_np\n",
    "\n",
    "      fetched = sess.run(fetches, feed_dict=feed_dict)\n",
    "\n",
    "      loss_np, tower_mems_np, curr_step = fetched[:3]\n",
    "      total_loss += loss_np\n",
    "\n",
    "      if curr_step > 0 and curr_step % model_config.iterations == 0:\n",
    "        curr_loss = total_loss / (curr_step - prev_step)\n",
    "        logging.info(\"[{}] | gnorm {:.2f} lr {:8.6f} \"\n",
    "            \"| loss {:.2f} | pplx {:>7.2f}, bpc {:>7.4f}\".format(\n",
    "            curr_step, fetched[-3], fetched[-2],\n",
    "            curr_loss, math.exp(curr_loss), curr_loss / math.log(2)))\n",
    "        total_loss, prev_step = 0., curr_step\n",
    "\n",
    "      if curr_step > 0 and curr_step % model_config.save_steps == 0:\n",
    "        save_path = os.path.join(model_config.model_dir, \"model.ckpt\")\n",
    "        saver.save(sess, save_path)\n",
    "        logging.info(\"Model saved in path: {}\".format(save_path))\n",
    "\n",
    "      if curr_step == model_config.train_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get corpus info\n",
    "corpus_info = get_corpus_info(model_config.corpus_info_path)\n",
    "n_token = corpus_info[\"vocab_size\"]\n",
    "cutoffs = corpus_info[\"cutoffs\"][1:-1]\n",
    "logging.info(\"n_token {}\".format(n_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
