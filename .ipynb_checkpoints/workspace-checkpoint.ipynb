{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-10-20 15:14:33.848425: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 15:14:34.525295: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-20 15:14:34.525343: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.4/lib64:\n",
      "2023-10-20 15:14:34.525349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Transformer_xl_config:\n",
    "    def __init__(self, data='/home/jun/workspace/wikitext-103/', dataset='wt103', n_layer=12, n_head=10,\n",
    "                 d_head=50, d_embed=-1, d_model=500, d_inner=1000, dropout=0.0, dropatt=0.0,\n",
    "                 init='normal', emb_init='normal', init_range=0.1, emb_init_range=0.01,\n",
    "                 init_std=0.02, proj_init_std=0.01, optim='adam', lr=0.00025, mom=0.0,\n",
    "                 scheduler='cosine', warmup_step=0, decay_rate=0.5, lr_min=0.0, clip=0.25,\n",
    "                 clip_nonemb=False, max_step=100000, batch_size=60, batch_chunk=1,\n",
    "                 tgt_len=70, eval_tgt_len=50, ext_len=0, mem_len=0, not_tied=False,\n",
    "                 seed=1111, cuda=False, adaptive=False, div_val=1, pre_lnorm=False,\n",
    "                 varlen=False, multi_gpu=False, log_interval=200, eval_interval=4000,\n",
    "                 work_dir='LM-TFM', restart=False, restart_dir='', debug=False,\n",
    "                 same_length=False, attn_type=0, clamp_len=-1, eta_min=0.0, gpu0_bsz=-1,\n",
    "                 max_eval_steps=-1, sample_softmax=-1, patience=0, finetune_v2=False,\n",
    "                 finetune_v3=False, fp16=False, static_loss_scale=1, dynamic_loss_scale=False,n_token=None):\n",
    "        \n",
    "        self.data = data\n",
    "        self.dataset = dataset\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.d_head = d_head\n",
    "        self.d_embed = d_embed\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dropout = dropout\n",
    "        self.dropatt = dropatt\n",
    "        self.init = init\n",
    "        self.emb_init = emb_init\n",
    "        self.init_range = init_range\n",
    "        self.emb_init_range = emb_init_range\n",
    "        self.init_std = init_std\n",
    "        self.proj_init_std = proj_init_std\n",
    "        self.optim = optim\n",
    "        self.lr = lr\n",
    "        self.mom = mom\n",
    "        self.scheduler = scheduler\n",
    "        self.warmup_step = warmup_step\n",
    "        self.decay_rate = decay_rate\n",
    "        self.lr_min = lr_min\n",
    "        self.clip = clip\n",
    "        self.clip_nonemb = clip_nonemb\n",
    "        self.max_step = max_step\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_chunk = batch_chunk\n",
    "        self.tgt_len = tgt_len\n",
    "        self.eval_tgt_len = eval_tgt_len\n",
    "        self.ext_len = ext_len\n",
    "        self.mem_len = mem_len\n",
    "        self.not_tied = not_tied\n",
    "        self.seed = seed\n",
    "        self.cuda = cuda\n",
    "        self.adaptive = adaptive\n",
    "        self.div_val = div_val\n",
    "        self.pre_lnorm = pre_lnorm\n",
    "        self.varlen = varlen\n",
    "        self.multi_gpu = multi_gpu\n",
    "        self.log_interval = log_interval\n",
    "        self.eval_interval = eval_interval\n",
    "        self.work_dir = work_dir\n",
    "        self.restart = restart\n",
    "        self.restart_dir = restart_dir\n",
    "        self.debug = debug\n",
    "        self.same_length = same_length\n",
    "        self.attn_type = attn_type\n",
    "        self.clamp_len = clamp_len\n",
    "        self.eta_min = eta_min\n",
    "        self.gpu0_bsz = gpu0_bsz\n",
    "        self.max_eval_steps = max_eval_steps\n",
    "        self.sample_softmax = sample_softmax\n",
    "        self.patience = patience\n",
    "        self.finetune_v2 = finetune_v2\n",
    "        self.finetune_v3 = finetune_v3\n",
    "        self.fp16 = fp16\n",
    "        self.static_loss_scale = static_loss_scale\n",
    "        self.dynamic_loss_scale = dynamic_loss_scale\n",
    "        self.n_token = n_token\n",
    "config = Transformer_xl_config()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, special=[], min_freq=0, max_size=None, lower_case=True,\n",
    "                 delimiter=None, vocab_file=None):\n",
    "        self.counter = Counter()\n",
    "        self.special = special\n",
    "        self.min_freq = min_freq\n",
    "        self.max_size = max_size\n",
    "        self.lower_case = lower_case\n",
    "        self.delimiter = delimiter\n",
    "        self.vocab_file = vocab_file\n",
    "\n",
    "    def tokenize(self, line, add_eos=False, add_double_eos=False):\n",
    "        line = line.strip() # 문자열 앞 뒤 공백제거\n",
    "        # convert to lower case\n",
    "        if self.lower_case:\n",
    "            line = line.lower()\n",
    "\n",
    "        # empty delimiter '' will evaluate False #구분자에 따라 문장을 분할 line = \"This is an example sentence.\"\n",
    "        #symbols = ['This', 'is', 'an', 'example', 'sentence.']\n",
    "\n",
    "        if self.delimiter == '':\n",
    "            symbols = line\n",
    "        else:\n",
    "            symbols = line.split(self.delimiter)\n",
    "\n",
    "        if add_double_eos: # lm1b\n",
    "            return ['<S>'] + symbols + ['<S>']\n",
    "        elif add_eos:\n",
    "            return symbols + ['<eos>']  #symbols = ['This', 'is', 'an', 'example', 'sentence.', '<eos>']\n",
    "\n",
    "        else:\n",
    "            return symbols\n",
    "\n",
    "    def count_file(self, path, verbose=False, add_eos=False):\n",
    "        if verbose: print('counting file {} ...'.format(path))\n",
    "        assert os.path.exists(path)\n",
    "\n",
    "        sents = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                    print('    line {}'.format(idx))\n",
    "                symbols = self.tokenize(line, add_eos=add_eos)\n",
    "                self.counter.update(symbols)\n",
    "                sents.append(symbols)\n",
    "\n",
    "        return sents\n",
    "\n",
    "    def count_sents(self, sents, verbose=False):\n",
    "        \"\"\"\n",
    "            sents : a list of sentences, each a list of tokenized symbols\n",
    "        \"\"\"\n",
    "        if verbose: print('counting {} sents ...'.format(len(sents)))\n",
    "        for idx, symbols in enumerate(sents):\n",
    "            if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                print('    line {}'.format(idx))\n",
    "            self.counter.update(symbols)\n",
    "\n",
    "    def _build_from_file(self, vocab_file):\n",
    "        self.idx2sym = []\n",
    "        self.sym2idx = OrderedDict()\n",
    "\n",
    "        with open(vocab_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                symb = line.strip().split()[0]\n",
    "                self.add_symbol(symb)\n",
    "        self.unk_idx = self.sym2idx['<UNK>']\n",
    "\n",
    "\n",
    "       \n",
    "    def build_vocab(self):  # 이 함수를 호출함으로써 문장 목록에 있는 단어들에 대하여 매칭 되는 숫자가 생기고 그 값이  self.sym2idx = OrderedDict() 에 들어가는 것\n",
    "        if self.vocab_file:\n",
    "            print('building vocab from {}'.format(self.vocab_file))\n",
    "            self._build_from_file(self.vocab_file)\n",
    "            print('final vocab size {}'.format(len(self)))\n",
    "        else:\n",
    "            print('building vocab with min_freq={}, max_size={}'.format(\n",
    "                self.min_freq, self.max_size))\n",
    "            self.idx2sym = []\n",
    "            self.sym2idx = OrderedDict()\n",
    "\n",
    "            for sym in self.special:\n",
    "                self.add_special(sym)\n",
    "\n",
    "            for sym, cnt in self.counter.most_common(self.max_size): # 빈도가 가장 높은 max size까지의(ex:5000)단어를 가져옴 sym 단어, cnt 카운트\n",
    "                if cnt < self.min_freq: break\n",
    "                self.add_symbol(sym)  \n",
    "\n",
    "            print('final vocab size {} from {} unique tokens'.format(\n",
    "                len(self), len(self.counter)))   \n",
    "\n",
    "    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n",
    "            add_double_eos=False):\n",
    "        if verbose: print('encoding file {} ...'.format(path))\n",
    "        assert os.path.exists(path)\n",
    "        encoded = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for idx, line in enumerate(f):\n",
    "                if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                    print('    line {}'.format(idx))\n",
    "                symbols = self.tokenize(line, add_eos=add_eos,\n",
    "                    add_double_eos=add_double_eos)\n",
    "                encoded.append(self.convert_to_tensor(symbols))\n",
    "\n",
    "        if ordered:\n",
    "            encoded = tf.concat(encoded, axis=0)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def encode_sents(self, sents, ordered=False, verbose=False):\n",
    "        if verbose: print('encoding {} sents ...'.format(len(sents)))\n",
    "        encoded = []\n",
    "        for idx, symbols in enumerate(sents):\n",
    "            if verbose and idx > 0 and idx % 500000 == 0:\n",
    "                print('    line {}'.format(idx))\n",
    "            encoded.append(self.convert_to_tensor(symbols))\n",
    "\n",
    "        if ordered:\n",
    "            encoded = tf.concat(encoded,axis=0)\n",
    "\n",
    "        return encoded\n",
    "\n",
    "    def add_special(self, sym):\n",
    "        if sym not in self.sym2idx:\n",
    "            self.idx2sym.append(sym)\n",
    "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "            setattr(self, '{}_idx'.format(sym.strip('<>')), self.sym2idx[sym])\n",
    "\n",
    "    def add_symbol(self, sym):\n",
    "        if sym not in self.sym2idx:\n",
    "            self.idx2sym.append(sym)\n",
    "            self.sym2idx[sym] = len(self.idx2sym) - 1\n",
    "    '''{           이렇게 sym2idx 에 단어랑 매칭 되는 숫자가 생긴다. \n",
    "    \"apple\": 0,\n",
    "    \"banana\": 1,\n",
    "    \"cherry\": 2\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    def get_sym(self, idx):\n",
    "        assert 0 <= idx < len(self), 'Index {} out of range'.format(idx)\n",
    "        return self.idx2sym[idx]\n",
    "\n",
    "    def get_idx(self, sym):\n",
    "        if sym in self.sym2idx:\n",
    "            return self.sym2idx[sym] # build voca를 통해 생긴 딕셔너리에서 단어 sym에 일치하는 숫자값을 리턴\n",
    "        else:\n",
    "            # print('encounter unk {}'.format(sym))\n",
    "            assert '<eos>' not in sym\n",
    "            assert hasattr(self, 'unk_idx')\n",
    "            return self.sym2idx.get(sym, self.unk_idx)\n",
    "\n",
    "    def get_symbols(self, indices):\n",
    "        return [self.get_sym(idx) for idx in indices]\n",
    "\n",
    "    def get_indices(self, symbols):\n",
    "        return [self.get_idx(sym) for sym in symbols]\n",
    "\n",
    "    def convert_to_tensor(self, symbols):\n",
    "        return tf.convert_to_tensor(self.get_indices(symbols), dtype=tf.float32)\n",
    "\n",
    "    def convert_to_sent(self, indices, exclude=None):\n",
    "        if exclude is None:\n",
    "            return ' '.join([self.get_sym(idx) for idx in indices])\n",
    "        else:\n",
    "            return ' '.join([self.get_sym(idx) for idx in indices if idx not in exclude])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2sym)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class LMOrderedIterator(object):\n",
    "    def __init__(self, data, bsz, bptt, device='/gpu:0', ext_len=None):\n",
    "        \"\"\"\n",
    "            data -- LongTensor -- the LongTensor is strictly ordered\n",
    "        \"\"\"\n",
    "        self.bsz = bsz #60\n",
    "        self.bptt = bptt #70\n",
    "        self.ext_len = ext_len if ext_len is not None else 0\n",
    "\n",
    "        self.device = device\n",
    "        self.data = data\n",
    "\n",
    "        \n",
    "        # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "        # 아래의 두 코드는   data 텐서에서 배치 크기 bsz로 깔끔하게 맞지 않는 추가 요소를 제거하는 것\n",
    "        self.n_step = self.data.shape[0] // self.bsz\n",
    "        # print(self.n_step)\n",
    "\n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        sliced_data = tf.slice(self.data,[0],[self.n_step * self.bsz])  \n",
    "        '''# 시작 위치와 슬라이싱할 크기 설정\n",
    "        begin = [0]  # 첫 번째 차원의 시작 위치는 0\n",
    "        size = [6]   # 첫 번째 차원에서 6개의 원소를 슬라이싱\n",
    "\n",
    "        # 데이터를 잘라내기 (tf.slice 사용)\n",
    "        sliced_data = tf.slice(data, begin, size)'''\n",
    "\n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        new_shape = (self.bsz, -1)  # 나머지 차원은 자동으로 계산됨\n",
    "        data_reshaped = tf.reshape(sliced_data, new_shape)\n",
    "        data_transposed = tf.transpose(data_reshaped)\n",
    "        '''\n",
    "        로,또,1,등,당,첨 = > 로,또,1    => 로, 등\n",
    "                        등,당,첨         또, 당\n",
    "                                        1, 첨\n",
    "        '''\n",
    "        '''\n",
    "        TensorFlow 2.x에서는 tf.device()를 사용하여 장치를 지정하는 방법이 아닙니다. 대신 tf.device()로 장치 컨텍스트를 설정한 후, 해당 컨텍스트 내에서 작업을 수행해야 합니다.\n",
    "        '''    \n",
    "        with tf.device(device):\n",
    "            self.data = data_transposed\n",
    "        \n",
    "        # self.data = data.view(bsz, -1).t().contiguous().to(device)\n",
    "\n",
    "        # Number of mini-batches\n",
    "        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n",
    "\n",
    "    def get_batch(self, i, bptt=None):\n",
    "        if bptt is None: bptt = self.bptt\n",
    "        seq_len = min(bptt, self.data.shape[0] - 1 - i) # # i값이 103227020를 넘지 않는 이상 seq_len = 70\n",
    "\n",
    "\n",
    "        end_idx = i + seq_len # 70,71,72,73,74......\n",
    "        beg_idx = max(0, i - self.ext_len) # 0,1,2,3,4,5\n",
    "        ''' 아래 처럼 첫번째 차원을 자르는 이류\n",
    "        로,또,1,등,당,첨 = > 로,또,1    => 로, 등\n",
    "                        등,당,첨         또, 당\n",
    "                                        1, 첨\n",
    "        '''\n",
    "        data = self.data[beg_idx:end_idx] # self.data[0:70],[1:71] ~\n",
    "        target = self.data[i+1:i+1+seq_len] #self.data[1:71],[2:72] ~\n",
    "\n",
    "        return data, target, seq_len\n",
    "\n",
    "    def get_fixlen_iter(self, start=0):\n",
    "        for i in range(start, self.data.shape[0] - 1, self.bptt):\n",
    "            yield self.get_batch(i) # 제너레이터 yield를  통해 만듬\n",
    "\n",
    "    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n",
    "        max_len = self.bptt + max_deviation * std\n",
    "        i = start\n",
    "        while True:\n",
    "            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n",
    "            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n",
    "            data, target, seq_len = self.get_batch(i, bptt)\n",
    "            i += seq_len\n",
    "            yield data, target, seq_len\n",
    "            if i >= self.data.size(0) - 2:\n",
    "                break\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.get_fixlen_iter() # tr_iter 등의 변수로 저장되며 변수는 호출 될 때마다 값을 하나씩 벹음\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class LMShuffledIterator(object):\n",
    "    def __init__(self, data, bsz, bptt, device='cpu', ext_len=None, shuffle=False):\n",
    "        \"\"\"\n",
    "            data -- list[LongTensor] -- there is no order among the LongTensors\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        print(self.data)\n",
    "        self.bsz = bsz\n",
    "        self.bptt = bptt\n",
    "        self.ext_len = ext_len if ext_len is not None else 0\n",
    "\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def get_sent_stream(self):\n",
    "        # index iterator\n",
    "        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle \\\n",
    "            else np.array(range(len(self.data)))\n",
    "\n",
    "        # sentence iterator\n",
    "        for idx in epoch_indices:\n",
    "            yield self.data[idx]\n",
    "\n",
    "    def stream_iterator(self, sent_stream):\n",
    "        # streams for each data in the batch\n",
    "        streams = [None] * self.bsz\n",
    "\n",
    "        data = tf.convert_to_tensor(self.bptt, self.bsz, dtype=tf.float32)\n",
    "        target = tf.convert_to_tensor(self.bptt, self.bsz, dtype=tf.float32)\n",
    "\n",
    "        n_retain = 0\n",
    "\n",
    "        while True:\n",
    "            # data   : [n_retain+bptt x bsz]\n",
    "            # target : [bptt x bsz]\n",
    "            data[n_retain:].fill_(-1)\n",
    "            target.fill_(-1)\n",
    "\n",
    "            valid_batch = True\n",
    "\n",
    "            for i in range(self.bsz):\n",
    "                n_filled = 0\n",
    "                try:\n",
    "                    while n_filled < self.bptt:\n",
    "                        if streams[i] is None or len(streams[i]) <= 1:\n",
    "                            streams[i] = next(sent_stream)\n",
    "                        # number of new tokens to fill in\n",
    "                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n",
    "                        # first n_retain tokens are retained from last batch\n",
    "                        data[n_retain+n_filled:n_retain+n_filled+n_new, i] = \\\n",
    "                            streams[i][:n_new]\n",
    "                        target[n_filled:n_filled+n_new, i] = \\\n",
    "                            streams[i][1:n_new+1]\n",
    "                        streams[i] = streams[i][n_new:]\n",
    "                        n_filled += n_new\n",
    "                except StopIteration:\n",
    "                    valid_batch = False\n",
    "                    break\n",
    "\n",
    "            if not valid_batch:\n",
    "                return\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            yield data, target, self.bptt\n",
    "\n",
    "            n_retain = min(data.size(0), self.ext_len)\n",
    "            if n_retain > 0:\n",
    "                data[:n_retain] = data[-n_retain:]\n",
    "            data.resize_(n_retain + self.bptt, data.size(1))\n",
    "\n",
    "    def __iter__(self):\n",
    "        # sent_stream is an iterator\n",
    "        sent_stream = self.get_sent_stream()\n",
    "\n",
    "        for batch in self.stream_iterator(sent_stream):\n",
    "            yield batch\n",
    "\n",
    "\n",
    "class LMMultiFileIterator(LMShuffledIterator):\n",
    "    def __init__(self, paths, vocab, bsz, bptt, device='cpu', ext_len=None,\n",
    "        shuffle=False):\n",
    "\n",
    "        self.paths = paths\n",
    "        self.vocab = vocab\n",
    "\n",
    "        self.bsz = bsz\n",
    "        self.bptt = bptt\n",
    "        self.ext_len = ext_len if ext_len is not None else 0\n",
    "\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def get_sent_stream(self, path):\n",
    "        sents = self.vocab.encode_file(path, add_double_eos=True)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(sents)\n",
    "        sent_stream = iter(sents)\n",
    "\n",
    "        return sent_stream\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.paths)\n",
    "\n",
    "        for path in self.paths:\n",
    "            # sent_stream is an iterator\n",
    "            sent_stream = self.get_sent_stream(path)\n",
    "            for batch in self.stream_iterator(sent_stream):\n",
    "                yield batch\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path, dataset, *args, **kwargs):\n",
    "        self.dataset = dataset\n",
    "        self.vocab = Vocab(*args, **kwargs)\n",
    "\n",
    "        if self.dataset in ['ptb', 'wt2', 'enwik8', 'text8']:\n",
    "            self.vocab.count_file(os.path.join(path, 'train.txt'))\n",
    "            self.vocab.count_file(os.path.join(path, 'valid.txt'))\n",
    "            self.vocab.count_file(os.path.join(path, 'test.txt'))\n",
    "        elif self.dataset == 'wt103':\n",
    "            self.vocab.count_file(os.path.join(path, 'train.txt'))\n",
    "        elif self.dataset == 'lm1b':\n",
    "            train_path_pattern = os.path.join(\n",
    "                path, '1-billion-word-language-modeling-benchmark-r13output',\n",
    "                'training-monolingual.tokenized.shuffled', 'news.en-*')\n",
    "            train_paths = glob.glob(train_path_pattern)\n",
    "            # the vocab will load from file when build_vocab() is called\n",
    "\n",
    "        self.vocab.build_vocab()\n",
    "\n",
    "        if self.dataset in ['ptb', 'wt2', 'wt103']:\n",
    "            self.train = self.vocab.encode_file(\n",
    "                os.path.join(path, 'train.txt'), ordered=True)    \n",
    "            self.valid = self.vocab.encode_file(\n",
    "                os.path.join(path, 'valid.txt'), ordered=True)\n",
    "            self.test  = self.vocab.encode_file(\n",
    "                os.path.join(path, 'test.txt'), ordered=True)\n",
    "        elif self.dataset in ['enwik8', 'text8']:\n",
    "            self.train = self.vocab.encode_file(\n",
    "                os.path.join(path, 'train.txt'), ordered=True, add_eos=False)\n",
    "            self.valid = self.vocab.encode_file(\n",
    "                os.path.join(path, 'valid.txt'), ordered=True, add_eos=False)\n",
    "            self.test  = self.vocab.encode_file(\n",
    "                os.path.join(path, 'test.txt'), ordered=True, add_eos=False)\n",
    "        elif self.dataset == 'lm1b':\n",
    "            self.train = train_paths\n",
    "            self.valid = self.vocab.encode_file(\n",
    "                os.path.join(path, 'valid.txt'), ordered=False, add_double_eos=True)\n",
    "            self.test  = self.vocab.encode_file(\n",
    "                os.path.join(path, 'test.txt'), ordered=False, add_double_eos=True)\n",
    "\n",
    "    def get_iterator(self, split, *args, **kwargs):\n",
    "        if split == 'train':\n",
    "            if self.dataset in ['ptb', 'wt2', 'wt103', 'enwik8', 'text8']:\n",
    "                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n",
    "            elif self.dataset == 'lm1b':\n",
    "                kwargs['shuffle'] = True\n",
    "                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n",
    "        elif split in ['valid', 'test']:\n",
    "            data = self.valid if split == 'valid' else self.test\n",
    "            if self.dataset in ['ptb', 'wt2', 'wt103', 'enwik8', 'text8']:\n",
    "                data_iter = LMOrderedIterator(data, *args, **kwargs)\n",
    "            elif self.dataset == 'lm1b':\n",
    "                data_iter = LMShuffledIterator(data, *args, **kwargs)\n",
    "\n",
    "        return data_iter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_lm_corpus(datadir, dataset):\n",
    "    fn = os.path.join(datadir, 'cache.pkl')\n",
    "    if os.path.exists(fn):\n",
    "        print('Loading cached dataset...')\n",
    "        with open(fn, 'rb') as file:\n",
    "            corpus = pickle.load(file)\n",
    "    else:\n",
    "        print('Producing dataset {}...'.format(dataset))\n",
    "        kwargs = {}\n",
    "        if dataset in ['wt103', 'wt2']:\n",
    "            kwargs['special'] = ['<eos>']\n",
    "            kwargs['lower_case'] = False\n",
    "        elif dataset == 'ptb':\n",
    "            kwargs['special'] = ['<eos>']\n",
    "            kwargs['lower_case'] = True\n",
    "        elif dataset == 'lm1b':\n",
    "            kwargs['special'] = []\n",
    "            kwargs['lower_case'] = False\n",
    "            kwargs['vocab_file'] = os.path.join(datadir, '1b_word_vocab.txt')\n",
    "        elif dataset in ['enwik8', 'text8']:\n",
    "            pass\n",
    "\n",
    "        corpus = Corpus(datadir, dataset, **kwargs)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        fn = os.path.join(datadir, \"cache.pkl\")\n",
    "\n",
    "        with open(fn, \"wb\") as fp:\n",
    "            pickle.dump(corpus, fp, protocol=2)\n",
    "            \n",
    "    return corpus\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "corpus = get_lm_corpus(config.data, config.dataset)\n",
    ""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading cached dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2023-10-20 15:15:32.718437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.718750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.722795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.723089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.723370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.723645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.724222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-20 15:15:32.873612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.873907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.874163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.874406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.874651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:32.874894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.452857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.453236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.453502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.453750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.453996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.454232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6627 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-20 15:15:33.454474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-10-20 15:15:33.454703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6627 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:02:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "len(corpus.vocab.idx2sym)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "267735"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "ntokens = len(corpus.vocab)\n",
    "n_token = ntokens\n",
    "config.n_token\n",
    "\n",
    "data_len = config.tgt_len * 20 # 20이 Segment의 갯수를 나타냅니다.\n",
    "device='gpu:0'\n",
    "\n",
    "eval_batch_size = 10\n",
    "tr_iter = corpus.get_iterator('train', config.batch_size, config.tgt_len,\n",
    "    device=device, ext_len=config.ext_len)\n",
    "va_iter = corpus.get_iterator('valid', eval_batch_size, config.eval_tgt_len,\n",
    "    device=device, ext_len=config.ext_len)\n",
    "te_iter = corpus.get_iterator('test', eval_batch_size, config.eval_tgt_len,\n",
    "    device=device, ext_len=config.ext_len)\n",
    ""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "value1=  next(tr_iter.get_fixlen_iter())\n",
    "data, target, seq_len = value1"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 }
}